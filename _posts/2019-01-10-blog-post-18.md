---
title: 'Quick Notes: Variation of GAN (Stacked GAN, StackGAN++, PA-GAN)'
date: 2019-01-15
permalink: /posts/2019/01/blog-post-18/
comments: true
tags:
  - DL
  - GAN
---

For the past 4 months, I have been working on cardiovascular disease risk prediction. Through this, I come up with an idea to utilize GAN to learn in a progressive way and decide to write a paper on this topic(Sry, I can't talk much about my idea in details.). Then, I began doing background research and found three related topic. In this post, I will give summarizations of these topic. 

From my point of view, stacked(or say progressive) GAN is kind of a new topic, related paper has been published starting from 2016 ( plz feel free to leave msg, if you know other related paper which is earlier than 2016, or you know other related interesting topics). 

**Table 1.**

| Paper(Topic)        | Publication Year | Goal  |
| ------------- |:-------------:| -----:|
| Stacked GAN      | 2017 | generate high-quality image |
| StackGAN++      | 2018      |  generate high-resolution photo image  |
| PA-GAN | 2019      |  stablize training process   |

# GAN
There are a lot of excellent blogs explaining the principles of GAN (e.g. [From GAN to WGAN](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html)). So I won't cover related details in this post. Before explaining the 3 GAN models, I'd like to recall the shortcomings of GAN. 

## Disadvantages of GAN
The training process can be viewed as a competition between Generator(G) and Discriminator(D). In my opinion, this can't reach a win-win situation. Imagine if G & D runs in a pretty tight race, half, then neither the simulated results of G would be of high-quality, nor the identification ability of D would be good enough. So there must be a winner between the two. Whether to choose G over D or otherwise, I think it depends on the specific goal of the project. 

About unstable training process and vanishing gradient, I think these two are somehow correlated, D is usually learning much well and faster than G, so its loss function gradient quickly becomes 0, meaning it can't provide accurate and continuous feedback to G. One solution is to enforce the model learning in a progressive process, which is the main idea in the following 3 paper.

About mode collapse: GAN has been mainly used for generating high quality simulated images, but its generations are usually limited within certain scope, meaning G can't produce diverse results. `Tips: In my opinion, the reason to input a random noise z to G is that z can alleviate mode collapse to certain extent, because of its randomness. (Plz correct me, if I'm not right.)`

<p float="left">
	<img src="/images/disGAN.png" width="400" />
</p>

**Figure 1. Disadvantages**

# Stacked Generative Adversarial Networks, SGAN

To me, the idea of SGAN is quite brilliant. Because instead of learning the whole data directly, it hierarchically extracts information from the data by using encoders, generate representation level by level, then let G & D learns in a top-down way, generating low-level info. conditioned on high-level representation. 

<p float="left">
	<img src="/images/SGAN1.png" width="400" />
</p>

**Figure 2. SGAN Framework**

It is stated in the paper that SGAN can generate much **high-quality** images than GAN. I believe this is because:

- By extracting information from images layer by layer(x-> h<sub>1</sub>-> ...-> h<sub>i</sub>->... -> y), the details have also been obtained. The adversarial loss enforces SGAN simulated intermediate representations are within the corresponding representation's manifold(In other words, ad loss ensures SGAN generated representations close to encoders' generated representations).

<p align="center">
	<img src="https://latex.codecogs.com/svg.latex?  L_{G_{2}}^{adv}(h_{i}, \widehat{h_{i}})" />
</p>

- By importing data level-by-level, this kind of controlling the speed of G/D's learning speed.

`These are all personal opinions, plz correct me, if something is wrong. Thx :)`

Next, I will dive into details of SGAN. The training stage can be divided into 3 sections:

**Table 2.**

| **Training Step**        | **Formula Details**    |**Loss Function**  | **Why Using This Loss Function**  |
| ------------- |:-------------:| -----:| ------------:|
| Encoders learn level-wise intermediate representations from bottom to top(Fig 3. left blue section, arrow direction) | ![](/images/SGANf1.png)| **Adversarial loss:** ![](/images/SGANf4.png)|Discriminator identify real data and generated ones, I would also regard this section as ensuring <img src="https://latex.codecogs.com/svg.latex?  \widehat{h_{i}}"> close to h<sub>i</sub> |
| Independent training of each G(Fig 3. central section)    | ![](/images/SGANf2.png) |  **Conditional loss:** ![](/images/SGANf5.png) |Impose G to use high-level conditional representation|
| Jointly end-to-end training of all Gs(Fig 3. central section, top to bottom, arrow direction)| ![](/images/SGANf3.png) | **entropy loss:** ![](/images/SGANf6.png)| Maintain diverse generations|
| **Testing Step**        | **Maintain Results Diversity**  |  |   |
| ------------- |:-------------:| -----:| ------------:|
| Top-to-bottom generating simulated images, no info. needed from encoder(Fig 3. right section)| By adding random noise z to each level|||


<p float="left">
	<img src="/images/SGAN2.png" width="1000" />
</p>

**Figure 3. SGAN Final Loss Function(Source: [1])**

I believe above summaries and tables help you walk through the principles of SGAN. There is one more tip I'd like to mention: **the 3rd training step.** In this step, SGAN tries to maximize result diversity, which is obtained by adding random noise z.
*Quick question: could you tell me how to prove the model indeed learns from this noise and uses it?*

Here is the answer: utilizing a conditional entropy <img src="https://latex.codecogs.com/svg.latex?  H(\widehat{h_{i}}|h_{i+1})"> to check if <img src="https://latex.codecogs.com/svg.latex?  \widehat{h_{i}}" />is sufficiently diverse when conditioned on h<sub>i+1</sub>.

{% capture notice-2 %}
**N:** # stacks, **h<sub>i</sub>:** level-wise representation, **x:** input images, h<sub>0</sub> = x, **y:** classification label, h<sub>N</sub> = y, **E<sub>i</sub>**, **G<sub>i</sub>**, **D<sub>i</sub>:** the ith encoder, generator, discriminator
{% endcapture %}
<div class="notice--info">{{ notice-2 | markdownify }}</div>


<p float="left">
	<img src="/images/SGAN2.png" width="300" />
</p>

**Figure 4. SGAN Framework Details(Source: [1])**

# StackGAN++

**TBA...**

# PA-GAN

**TBA...**

<p float="center"><img src="https://latex.codecogs.com/svg.latex?  " title="p" /></p>


{% capture notice-2 %}
{% endcapture %}
<div class="notice--warning">{{ notice-2 | markdownify }}</div>


{% capture notice-2 %}
{% endcapture %}
<div class="notice--info">{{ notice-2 | markdownify }}</div>


Reference
========

[1]. Xun H., Yixuan L. et al. Stacked Generative Adversarial Networks, CVPR(2017)

[2]. [SGAN Github Repo](https://github.com/xunhuang1995/SGAN)

[3]. https://medium.com/@jonathan_hui/gan-stacked-generative-adversarial-networks-sgan-d9449ac63db8

[4]. Han Zh., Tao X. et al. StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks

[5]. PA-GAN: Improving GAN Training by Progressive Augmentation


------