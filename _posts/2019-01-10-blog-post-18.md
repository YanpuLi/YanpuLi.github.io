---
title: 'Quick Notes: Variation of GAN (Stacked GAN, StackGAN++, PA-GAN)'
date: 2019-01-15
permalink: /posts/2019/01/blog-post-18/
comments: true
tags:
  - DL
  - GAN
---

For the past 4 months, I have been working on cardiovascular disease risk prediction. Through this, I come up with an idea to utilize GAN to learn in a progressive way and decide to write a paper on this topic(Sry, I can't talk much about my idea in details.). Then, I began doing background research and found three related topic. In this post, I will give summarizations of these topic. 

From my point of view, stacked(or say progressive) GAN is kind of a new topic, related paper has been published starting from 2016 ( plz feel free to leave msg, if you know other related paper which is earlier than 2016, or you know other related interesting topics). 

**Table 1.**

| Paper(Topic)        | Publication Year | Goal  |
| ------------- |:-------------:| -----:|
| Stacked GAN      | 2017 | generate high-quality image |
| StackGAN++      | 2018      |  generate high-resolution photo image  |
| PA-GAN | 2019      |  stablize training process   |

# GAN
There are a lot of excellent blogs explaining the principles of GAN (e.g. [From GAN to WGAN](https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html)). So I won't cover related details in this post. Before explaining the 3 GAN models, I'd like to recall the shortcomings of GAN. 

## Disadvantages of GAN
The training process can be viewed as a competition between Generator(G) and Discriminator(D). In my opinion, this can't reach a win-win situation. Imagine if G & D runs in a pretty tight race, half, then neither the simulated results of G would be of high-quality, nor the identification ability of D would be good enough. So there must be a winner between the two. Whether to choose G over D or otherwise, I think it depends on the specific goal of the project. 

About unstable training process and vanishing gradient, I think these two are somehow correlated, D is usually learning much well and faster than G, so its loss function gradient quickly becomes 0, meaning it can't provide accurate and continuous feedback to G. One solution is to enforce the model learning in a progressive process, which is the main idea in the following 3 paper.

About mode collapse: GAN has been mainly used for generating high quality simulated images, but its generations are usually limited within certain scope, meaning G can't produce diverse results. `Tips: In my opinion, the reason to input a random noise z to G is that z can alleviate mode collapse to certain extent, because of its randomness. (Plz correct me, if I'm not right.)`

<p float="left">
	<img src="/images/disGAN.png" width="400" />
</p>

**Figure 1. Disadvantages**

# Stacked Generative Adversarial Networks, SGAN

To me, the idea of SGAN is quite brilliant. Because instead of learning the whole data directly, it hierarchically extracts information from the data by using encoders, generate representation level by level, then evaluates the quality of each representation to ensure the info. has been correctly learned. Finally, let G & D learns from these information. 

It is stated in the paper that SGAN can generate much **high-quality** images than GAN. I believe this is because:

1. By extracting information from images layer by layer(x-> h<sub>1</sub>-> ...-> h<sub>i</sub>->... -> y), the details have also been obtained.

2. The quality of each level's representation has been evaluated to make sure the extracted info. is of high quality.

(<p float="center"><img src="https://latex.codecogs.com/svg.latex?  L_{G_{2}}^{adv}(h_{i}, \widehat{h_{i}})" title="p" /></p>)

3. By importing data level-by-level, this kind of controlling the speed of G/D's learning speed.

`These are all personal opinions, plz correct me, if something is wrong. Thx :)`

Next, I will dive into details of SGAN.

<p float="left">
	<img src="/images/SGAN.png" width="800" />
</p>

**Figure 2. SGAN Framework**

# StackGAN++

**TBA...**

# PA-GAN

**TBA...**

<p float="center"><img src="https://latex.codecogs.com/svg.latex?  " title="p" /></p>


{% capture notice-2 %}
{% endcapture %}
<div class="notice--warning">{{ notice-2 | markdownify }}</div>


{% capture notice-2 %}
{% endcapture %}
<div class="notice--info">{{ notice-2 | markdownify }}</div>


**TBA..**

Reference
========

[1]. Xun H., Yixuan L. et al. Stacked Generative Adversarial Networks, CVPR(2017)

[2]. [SGAN Github Repo](https://github.com/xunhuang1995/SGAN)

[3]. Han Zh., Tao X. et al. StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks

[4]. PA-GAN: Improving GAN Training by Progressive Augmentation


------