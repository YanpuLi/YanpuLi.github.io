---
title: 'SVM: Support Vector Machine'
date: 2015-11-21
permalink: /posts/2015/11/blog-post-2/
---
The key of SVM is to find a hyperplane, which is built on some important instances (support vectors), to seperate data instances correctly. Here comes with a very contradictory process to construct the plane: the margin of the hyperplane is chosen to be <ins>the smallest distance </ins>between decision boundary and support vectors; at the same time the decision boundary need to be the one which the margin is <ins>maximized</ins>. This is because there can be many hyperplanes (Fig. 1) to seperate data correctly. Choosing the one which leads to the largest gap between both classes may be more resistant to any perturbation of the training data. 

<p float="left">
  <img src="/images/svm_multiple_hyperplanes.png" width="250" />
  <img src="/images/svm_maximum_margin.png" width="250" /> 
</p>
### Figure 1. infinite hyperplanes and maximum margin

Maximum Margin Classifier
======

Let's take a two-class classification problem using linear models as the example.  

y(x) = **w**<sup>T</sup>φ(x) + b                                                                                                

**W**: a weight vector; φ(x): input vector; b: bias; training set: N input vectors x<sub>1</sub>,..., x<sub>N</sub>, with corresponding target values t<sub>1</sub>,..., t<sub>N</sub> where t<sub>n</sub> ∈{−1,1}; so the decision boundary can be represented as **w**<sup>T</sup>φ(x) + b = 0; for the 2 classes, one satisfies y(x<sub>n</sub>) > 0, meaning t<sub>n</sub> = 1; the other satisfies y(x<sub>n</sub>) < 0, meaning t<sub>n</sub> = -1, so that t<sub>n</sub>*y(x<sub>n</sub>) > 0;   
The distance of a point x<sub>n</sub> to this decision boundary is defined as:  t<sub>n</sub>*y(x<sub>n</sub>) / ||w||            

So the problem becomes:     
<p float="left"><img src="/images/svm_formula3.png" width="300" /></p>                     

If we change w → κw and b → κb, the distance from x<sub>n</sub> to the decision boundary is unchanged. So we set this freedom to 
t<sub>n</sub> (w<sup>T</sup>φ(x<sub>n</sub>) + b)= 1; so all training set will meet the constraint: t<sub>n</sub> (w<sup>T</sup>φ(x<sub>n</sub>)+b) >= 1, n=1,...,N.

Then the problem can be changed into: 
<p float="left"><img src="/images/svm_formula4.png" width="160" /> </p>    
s.t. t<sub>n</sub> (w<sup>T</sup>φ(x<sub>n</sub>)+b) >= 1, n=1,...,N.

Introducing Lagrange multiplier a<sub>n</sub> >= 0, to solve this constrained optimization problem:
<p float="left"><img src="/images/svm_formula5.png" width="400" /></p>  

Then, set the derivatives of L(w, b, a) with respect to w, b to 0, we get 2 conditions:
<p float="left"><img src="/images/svm_formula6.png" width="170" /></p>  
Put these 2 conditions back into previous formula: 
<p float="left"><img src="/images/svm_formula7.png" width="400" /></p>  
s.t.
<p float="left"><img src="/images/svm_formula8.png" width="250" /></p>  
##Soft Margin: 
Introducing slack variable ξ<sub>n</sub> to allow misclassified data:
ξ<sub>n</sub> = 0, represents data on the correct side of margin, or correctly classified;
0 < ξ<sub>n</sub> <= 1, means data inside of margin, but on correct side of decision boundary;
ξ<sub>n</sub> > 1, denotes data on the wrong side of decision boundary.
Thus, we want to minimize:
<p float="left"><img src="/images/svm_formula9.png" width="150" /></p> 
where C > 0, it controls the trade-off between the slack variable and the margin. Then, the Lagrangian formula is defined as:
<p float="left"><img src="/images/svm_formula10.png" width="450" /></p> 
With KKT conditions:
<p float="left"><img src="/images/svm_formula11.png" width="180" /></p> 
Again set the derivatives with respect to w, b, ξ<sub>n</sub> to 0, then we get, and obtain the new form of Lagrangian, in which we maximize it with respect to a:
<p float="left"><img src="/images/svm_formula12.png" width="220" /></p> 
<p float="left"><img src="/images/svm_formula13.png" width="380" /></p> 
s.t. 
<p float="left"><img src="/images/svm_formula14.png" width="100" /></p> 
Then, we can obtain b by averaging:
<p float="left"><img src="/images/svm_formula15.png" width="330" /></p> 

**Inner Product:** 
In previous formula, k(x<sub>n</x<sub>, x<sub>m</sub>) can be viewed as the inner product of two vectors. 
* If x<sub>n</x<sub>, x<sub>m</sub> are parallel, there inner product is maximized.
* If x<sub>n</x<sub>, x<sub>m</sub> are perpendicular, then their inner product is 0.

Let's take a look back at the Lagrangian formula. Being perpendicular means two features are completely different, as there product is 0, it won't influence the value of L. However, if they are parallel, meaning x<sub>n</x<sub>, x<sub>m</sub>, both features are completely alike. if 2 features predict the same class output, then a<sub>n</sub>a<sub>m</sub>t<sub>n</sub>t<sub>m</sub>k<x<sub>n</x<sub>, x<sub>m</sub>> will be positive, meaning L will decrease; if 2 features gives the opposite predictions, then a<sub>n</sub>a<sub>m</sub>t<sub>n</sub>t<sub>m</sub>k(x<sub>n</x<sub>, x<sub>m</sub>) is negative, so the value of L will increase. This is the situation we want most, as it can tell the 2 classes apart.




Table 1

| url           | label   |    
| ---------        | ------ | 
| iamagameaddict.com     | bad  | 
| slightlyoffcenter.net    | bad   | 

About the format of a url: it contains protocol, host name (primary domain) and so on. So in the 1st step, we need to cut a whole url into Tokens 



Aren't headings cool?
------

classification evaluation

recommendation system





reinforcement learning

data transformation

Cellular-Automata-Rule30-Implementation

http://www.cnblogs.com/90zeng/p/Lagrange_duality.html
http://blog.csdn.net/stdcoutzyx/article/details/9774135
http://logos.name/archives/304
http://shenchao.me/