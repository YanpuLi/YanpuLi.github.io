---
title: 'SVM: Support Vector Machine'
date: 2015-11-21
permalink: /posts/2015/11/blog-post-2/
---
The key of SVM is to find a hyperplane, which is built on some important instances (support vectors), to seperate data instances correctly. Here comes with a very contradictory process to construct the plane: the margin of the hyperplane is chosen to be <ins>the smallest distance between decision boundary and support vectors</ins>; at the same time the decision boundary need to be the one which the margin is <ins>maximized</ins>. This is because there can be many hyperplanes (Fig. 1) to seperate data correctly. Choosing the one which leads to the largest gap between both classes may be more resistant to any perturbation of the training data. 

<p float="left">
  <img src="/images/svm_multiple_hyperplanes.png" width="250" />
  <img src="/images/svm_maximum_margin.png" width="250" /> 
</p>
### Figure 1. infinite hyperplanes and maximum margin

Maximum Margin Classifier
======

Let's take a two-class classification problem using linear models as the example.  

(1). y(x) = **w**<sup>T</sup>φ(x) + b                                                                                                

**W**: a weight vector; φ(x): input vector; b: bias; training set: N input vectors x<sub>1</sub>,..., x<sub>N</sub>, with corresponding target values t<sub>1</sub>,..., t<sub>N</sub> where t<sub>n</sub> ∈{−1,1}; so the decision boundary can be represented as **w**<sup>T</sup>φ(x) + b = 0; for the 2 classes, one satisfies y(x<sub>n</sub>) > 0, meaning t<sub>n</sub> = 1; the other satisfies y(x<sub>n</sub>) < 0, meaning t<sub>n</sub> = -1, so that t<sub>n</sub>*y(x<sub>n</sub>) > 0;   
The distance of a point x<sub>n</sub> to this decision boundary is defined as:  t<sub>n</sub>*y(x<sub>n</sub>) / ||w||            

So the problem becomes:    
(2). 
<p float="left"><img src="/images/svm_formula3.png" width="300" /></p>                     

If we change w → κw and b → κb, the distance from x<sub>n</sub> to the decision boundary is unchanged. So we set this freedom to 
t<sub>n</sub> (w<sup>T</sup>φ(x<sub>n</sub>) + b)= 1; so all training set will meet the constraint: t<sub>n</sub> (w<sup>T</sup>φ(x<sub>n</sub>)+b) >= 1, n=1,...,N.

Then the problem can be changed into:
(3). 
<p float="left"><img src="/images/svm_formula4.png" width="160" /> </p>    
s.t. t<sub>n</sub> (w<sup>T</sup>φ(x<sub>n</sub>)+b) >= 1, n=1,...,N.

Introducing Lagrange multiplier a<sub>n</sub> >= 0, to solve this constrained optimization problem:
(4). 
<p float="left"><img src="/images/svm_formula5.png" width="400" /></p>  

Then, set the derivatives of L(w, b, a) with respect to w, b to 0, we get 2 conditions:
<p float="left"><img src="/images/svm_formula6.png" width="170" /></p>  
Put these 2 conditions back into formula (4):
(5). 
<p float="left"><img src="/images/svm_formula7.png" width="400" /></p>  
s.t.
<p float="left"><img src="/images/svm_formula8.png" width="250" /></p>  









Table 1

| url           | label   |    
| ---------        | ------ | 
| iamagameaddict.com     | bad  | 
| slightlyoffcenter.net    | bad   | 

About the format of a url: it contains protocol, host name (primary domain) and so on. So in the 1st step, we need to cut a whole url into Tokens 



Aren't headings cool?
------

classification evaluation

recommendation system





reinforcement learning

data transformation

Cellular-Automata-Rule30-Implementation

http://www.cnblogs.com/90zeng/p/Lagrange_duality.html
http://blog.csdn.net/stdcoutzyx/article/details/9774135
http://logos.name/archives/304
http://shenchao.me/