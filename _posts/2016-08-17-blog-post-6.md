---
title: 'Very Confusing Pairs to Me🤔😥😴'
date: 2016-08-17
permalink: /posts/2016/08/blog-post-6
---
This post is used for elaborating details and make comparisons of some very similar and confusing pairs to me.


Bias & Variance
======
Bias is an error from erroneous assumptions in the learning algorithm. Variance is an error from sensitivity to small fluctuations in the training set. This is the definition from Wiki. But it is still kind of confusing. To me, bias measures how far your prediction is away from the true value. Variance measures how far your prediction is away from the mean value, meaning how scattered your prediction is. Here is a very famous visualization of these two items. You can easily get my points from it.
![bias&var](/images/biasvar1.png)
###Figure 1. Bias and Variance Visualization

Then, let's take a look at the mathematical definitions. Assuming a relationship between a covariate X and Y, let Y = f(X) + ϵ, where ϵ follows a normal distribution, ϵ∼N(0,σ<sub>ϵ</sub>), f̂(X) denotes the estimated model.
In this case, the mean squared error of a point x is:
<p float="left">
  <img src="/images/equation.png" width="300" />
</p>
So the equation is composed of 3 parts: variance, bias and irreducible error which is the noise term in the true relationship and can't be reduced by any model.

![bias&var](/images/biasvar2.png)

Cost Function & Loss Function
======

L1 & L2
======

Classification Evaluation
======

###Specificity & Sensitivity

###Precision & Recall

**To be continued...**

Reference
------
[1]. http://scott.fortmann-roe.com/docs/BiasVariance.html
[2].
[3].
[4]. https://en.wikipedia.org/wiki/Sensitivity_and_specificity


