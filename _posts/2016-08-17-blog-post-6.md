---
title: 'Very Confusing Pairs to Meヰ'
date: 2016-08-17
permalink: /posts/2016/08/blog-post-6
---
This post is used for elaborating details and make comparisons of some very similar and confusing pairs to me.


Bias & Variance
======
Bias is an error from erroneous assumptions in the learning algorithm. Variance is an error from sensitivity to small fluctuations in the training set. This is the definition from Wiki. But it is still kind of confusing. To me, bias measures how far your prediction is away from the true value. Variance measures how far your prediction is away from the mean value, meaning how scattered your prediction is. Here is a very famous visualization of these two items. You can easily get my points from it.
![bias&var](/images/biasvar1.png)
Figure 1. Bias and Variance Visualization

Then, let's take a look at the mathematical definitions. Assuming a relationship between a covariate X and Y, let Y = f(X) + 系, where 系 follows a normal distribution, 系N(0,<sub>系</sub>), f(X) denotes the estimated model.
In this case, the mean squared error of a point x is:
<p float="left"><img src="/images/equation.jpg" width="500" /></p>
So the equation is composed of 3 parts: variance, bias and irreducible error which is the noise term in the true relationship and can't be reduced by any model.

Tradeoff between variance and bias: With the increase in model complexity, the bias drops down and var goes up. At the same time, it will lead to overfitting, meaning your model performs too well on the training set, but poorly on testing data. Keep your model simple or introducing regularization are the methods to control bias. For variance, you can construct your data with resampling methods, then multiple models would be generated on these datasets, and finally got averaged to reduce variance.
<p float="left"><img src="/images/biasvar2.png" width="400" /></p>
Figure 2. Bias & Var Tradeoff

Cost Function & Loss Function
======
Both functions deal with training error. In Andrew Ng's Neural Networks and Deep Learning course, he gives very specific definition of the two: the loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set. Then, to optimize the loss function or cost function), we can get the objective function, which is composed of empirical risk (related with loss/ cost function) and structural risk (related with regularization).


L1 & L2
======
L1-norm loss function tries to minimize the sum of absolute error between the targets and predictions. while L2-norm loss function changes absolute value into squared one. If the error > 1, then L2 will make the error become larger than L1. In this way, L2 is more sensitive than L1 on outliers.

Regularization is used to prevent overfitting by introducing a regularization term in the objective function to add penalty. L1 (lasso) is the sum of weights and l2 is the sum of square of weights. L1 can change large coefficients into 0, while L2 can only make the coefficient close to 0. Hence, L1 can works as a feature selection process. 
<p float="left"><img src="/images/l1l2.png" width="350" /></p>
Figure 3. L1 & L2

Classification Evaluation
======
<p float="left"><img src="/images/classe.png" width="380" /></p>
Figure 3. Diagnostic Testing Matrix
**Specificity & Sensitivity:** Sensitivity (recall) meansures the proportion of positives that are correctly identified. Specificity measures how good a test is at avoiding false alarms.
**Precision & Recall:** Precision tells the fraction of retrieved instances that are relevant. Recall calculates the fraction of relevant instances which have been retrieved.
<p float="left"><img src="/images/recall.png" width="200" /></p>
So when make predictions of earthquake, we don't mind to make too many alarms. Because our goal is to minimize the casualties, we want to have higher recall at the expense of precision. It will become a different when finding criminals. We will try to keep a high precision value. This is very understandable, as we don't want to make troubles to the innocent ones.

**To be continued...**

Reference
------
[1]. http://scott.fortmann-roe.com/docs/BiasVariance.html

[2]. https://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing

[3]. http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/

[4]. https://en.wikipedia.org/wiki/Sensitivity_and_specificity

[5]. http://mlwiki.org/index.php/Precision_and_Recall


