---
title: 'Logistic Regression'
date: 2016-05-15
permalink: /posts/2016/05/blog-post-3/
---
## Introduction

Logistic regression is to directly estimate the distribution of a conditional probability model from the training set, and obtain the unknown parameters by using maximum likelihood estimation. It assumes the data is linearly seperable. For example, a 2D dataset, it can be seperate by a linear decision boundary, which is $wX+b=0$. If a point makes $w^Tx+b>0$, then it is more likely belongs to class 1, otherwise, class 0. The process to achieve such decision boundary can be viewed as studying the relationship between $P(Y=1)$ and the input $X$, and use this probability to identify the class. A sigmoid function $g(z) = \frac{1}{1+e^{-z}}$ can perfectly meet the requirements.

1. it assures an output range from 0 to 1.
2. the odds $log{P(Y=1|x)\over P(Y=0|x)}$ is equal to $w^Tx+b$

<p float="left"><img src="/images/lg1.png" width="220" /></p>

**Figure 1.** Sigmoid Function

The model can be represented as:
* $g(z) = \frac{1}{1+e^{-z}}$
* $P(Y=1 | x) = g(w^Tx+b)$
* $P(Y=0 | x) = 1 - P(Y=1 | x) = 1 - g(w^Tx+b)$
* $P(Y=y_k | x;w;b) = P(Y=1 | x)^{y_k}(1 - P(Y= 1| x))^{1-y_k}$
      
**MLE**
---
Assuming all the training set (N samples) were generated independently, so the likelihood of the parameters can be represented as:$$MLE(w,b) = \pi^N_{n=1}  P(Y=1 | x)^{y_k}(1 - P(Y= 1| x))^{1-y_k}$$

Then, we want to obtain w, b which maximize the log likelihood:$$MLE(w,b) = \sum^N_{n=1} {y_k} log P(Y=1 | x) + (1-y_k)log(1 - P(Y= 1| x))$$

Correspondingly, we try to find w, b to minimize the negative log-likelihood for logistic regression:
$$NLL(w,b) = -\sum^N_{n=1} [{y_k} log P(Y=1 | x) + (1-y_k)log(1 - P(Y= 1| x))]$$
This is in the form of cross-entropy error function. So, here comes with the error function for Logistic Regression

**Loss Function & Cost Function**
---
In linear regression, we use squared error to measure the loss between y and $\hat y$ for each sample, and sum of squared error to calculate cost function for the whole dataset. While in logistic regression,  
Loss function: $L(y, \hat y) = -[{y_k} log P(Y=1 | x) + (1-y_k)log(1 - P(Y= 1| x))]$

Cost function: $J(w, b) = \frac {1} {m} { \sum^N_{n=1} L(y, \hat y)}$

In the training process, the goal is to minimize the $E_{in}(W)$, which means to find a spot, where $▿E_{in}(W)=0$, when $E_{in}(W)$ is a continuous convex function. But unlike linear regression, whose $▿E_{in}(W)$ is linear, it is hard for us to calculate $▿E_{in}(W)$ for LR directly. So, we use an optimization algorithm instead, which is gradient descent.

## Gradient Descent

To obtain optimal w, b by using gradient descent, we need to iteratively takes steps in the direction of negative of the gradient, update the weights, until certain requirements have been met(e.g. $||J(w^{k+1})−J(w^k)|| < ϵ, or  ||w^{k+1}−w^k||< ϵ$ ). As the conditional likelihood for logistic regression is concave, we can find the global minimum rather than a local minimum.

Repeat:

$w:= w- α\frac{∂J(w,b)}{∂w}$,

$b:= b- α\frac{∂J(w,b)}{∂b}$
    
where α is a positive constant learning rate to control the size of each step.  

**Regularization** 
---
Regularization is used for reduce overfitting by adding penalty for large values of $w$. 
$J(w) = -\frac {1} {m} \sum^N_{n=1}[{y_k} log P(Y=1 | x) + (1-y_k)log(1 - P(Y= 1| x)] + λΦ(w)$

One way is to add L2 (ridge) norm to the log likelihood:

$Φ(w) = \sum^M_{m=1}{w^2_m}$ (λ would be set as $\frac{λ}{2}$)

The other is using L1 (lasso) norm:

$Φ(w) = |w|$

L2-regularized loss function is smooth, meaning the optimum is the stationary point (0-derivative point). L2 can be regarded as weight decay, it won't make the coefficients become zero. On the contrary, L1 can. Meanwhile, L1 regularization gives you sparse estimates.

Experiment: Malicious URL Dection
======
The dataset contains around 420,000 pieces url records. We want to identify if it's a bad url.

Table 1

| url           | label   |    
| ---------        | ------ | 
| iamagameaddict.com     | bad  | 
| slightlyoffcenter.net    | bad   | 

So the 1st step is to cut the url into tokens. As the format of a url is kind of fixed: it contains protocol, host name (primary domain) and so on, we can firstly cut the url by '.', '/', and remove some common parts in the url, such as 'http', 'com', 'net'. Then, we would use logistic regression to do the prediction.

<p float="left"><img src="/images/lg11.png" width="280" /></p>

I have used gridsearchCV to find optimal parameters which is l2, with C (equal to 1/λ) set to be 100, then the classificatio accuracy of training set is around 97%, while for the testing set, it can also be up to 95%.

<p float="left"><img src="/images/lg12.png" width="280" /><img src="/images/lg13.png" width="280" /></p>

References
------
[1]. BISHOP, C. M. (2016). PATTERN RECOGNITION AND MACHINE LEARNING. S.l.: SPRINGER-VERLAG NEW YORK.

[2]. Murphy, K. P. (2013). Machine learning: a probabilistic perspective. Cambridge, Mass.: MIT Press.

[3]. http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji/

[4]. https://chenrudan.github.io/blog/2016/01/09/logisticregression.html

[5]. http://www.csuldw.com/2016/09/19/2016-09-19-logistic-regression-theory/

[6]. https://hackernoon.com/gradient-descent-aynk-7cbe95a778da

<img src="https://latex.codecogs.com/svg.latex?\Large&space;x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}" title="\Large x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}" />

\begin{equation}
\sum
\end{equation}



$ \sum_{\forall i}{x_i^{2}} $

