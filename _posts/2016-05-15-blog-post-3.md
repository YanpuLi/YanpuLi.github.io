---
title: 'Logistic Regression'
date: 2016-05-15
permalink: /posts/2016/05/blog-post-3/
tags:
  - ML
---

## Introduction
Logistic regression uses sigmoid function to estimate the probability of a sample belonging to a certain class, and obtains the unknown parameters by using maximum likelihood estimation. It assumes the data is linearly seperable as linear regression does. For example, a 2D dataset, it can be seperate by a linear decision boundary, which is wX+b=0. If a point makes wx+b>0, then it is more likely belongs to class 1, otherwise, class 0. 

**Why Sigmoid?**
---

Because the sigmoid function g(z) = 1/(1+e<sup>-z</sup>), z = W<sup>T</sup>x+b can perfectly meet the following requirements:
1. it assures the output ranges from 0 to 1 (probability).
2. the log odds: <img src="https://latex.codecogs.com/svg.latex?log{P(Y=1|x)\over P(Y=0|x)}" title="p" /> is equal to w<sup>T</sup>x+b (the decision boundary)

The model can be represented as:
* <img src="https://latex.codecogs.com/svg.latex?g(z) = \frac{1}{1+e^{-z}}" title="p" />
* P(Y=1 \| x) = g(w<sup>T</sup>x+b)
* P(Y=0 \| x) = 1 - P(Y=1 \| x) = 1 - g(w<sup>T</sup>x+b)
* P(Y=y<sub>k</sub> \| x;w;b) = P(Y=1 \| x)<sup>y<sub>k</sub></sup>(1 - P(Y= 1\| x))<sup>1-y<sub>k</sub></sup>

<p float="left"><img src="/images/p3sigmoid.png" width="320" /></p>

**Figure 1.** Sigmoid Function & Decision Boundary _(Image source: [ML Cheatsheet](https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html#cost-function))_

**Parameter Estimation: MLE**
---
Assuming all the training set (N samples) were generated independently, so the likelihood of the parameters can be represented as:  <img src="https://latex.codecogs.com/svg.latex?MLE(w,b) =\pi^N_{n=1}  P(Y=1 | x)^{y_k}(1 - P(Y= 1| x))^{1-y_k}" title="p" /> 

Then, we want to obtain w, b which maximize the log likelihood:
<img src="https://latex.codecogs.com/svg.latex?MLE(w,b) = \sum^N_{n=1} {y_k} log P(Y=1 | x) + (1-y_k)log(1 - P(Y= 1| x))" title="p" /> 

Correspondingly, we try to find w, b to minimize the negative log-likelihood for logistic regression:
<img src="https://latex.codecogs.com/svg.latex?NLL(w,b) = -\sum^N_{n=1} [{y_k} log P(Y=1 | x) + (1-y_k)log(1 - P(Y= 1| x))]" title="p" /> 
This is in the form of cross-entropy error function. So, here comes with the error function for Logistic Regression

**Loss Function & Cost Function**
---
In linear regression, squared error is used to measure the loss between y and <img src="https://latex.codecogs.com/svg.latex?\hat y" title="p" /> for each sample, and sum of squared error to calculate cost function for the whole dataset. While in logistic regression, cross entropy is the substitution.

Loss function: <img src="https://latex.codecogs.com/svg.latex?L(y, \hat y) = -[{y_k} log P(Y=1 | x) + (1-y_k)log(1 - P(Y= 1| x))]" title="p" />

Cost function: <img src="https://latex.codecogs.com/svg.latex?J(w, b) = \frac {1} {m} { \sum^N_{n=1} L(y, \hat y)}" title="p" />

In the training process, the goal is to minimize the E<sub>in</sub>(W), which means to find a spot where ▿E<sub>in</sub>(W)=0, when E<sub>in</sub>(W) is a continuous convex function. But unlike linear regression, whose ▿E<sub>in</sub>(W) is linear, it is hard to calculate ▿E<sub>in</sub>(W) for LR directly. So that's where gradient descent is introduced.

## Gradient Descent

To obtain optimal w, b by using gradient descent, we need to iteratively takes steps in the direction of negative of the gradient, update the weights, until certain requirements have been met(e.g.  \|\|w^(k+1)−w^k\|\|< ϵ  or [J(w^(k+1)−J(w^k)]< ϵ ). As the conditional likelihood for logistic regression is concave, we can find the global minimum rather than a local minimum.

Repeat:

<img src="https://latex.codecogs.com/svg.latex?w:= w- α\frac{∂J(w,b)}{∂w}" title="p" />,

<img src="https://latex.codecogs.com/svg.latex?b:= b- α\frac{∂J(w,b)}{∂b}" title="p" />
    
where α is a positive constant learning rate to control the size of each step.  

**Regularization** 
---
Regularization is used for reduce overfitting by adding penalty for large values of w. 
<img src="https://latex.codecogs.com/svg.latex?J(w) = -\frac {1} {m} \sum^N_{n=1}[{y_k} log P(Y=1 | x) + (1-y_k)log(1 - P(Y= 1| x)]" title="p" />+ λΦ(w)

One way is to add L2 (ridge) norm to the log likelihood:

Φ(w) =<img src="https://latex.codecogs.com/svg.latex?\sum^M_{m=1}{w^2_m}" title="p" /> (λ would be set as λ/2)

The other is using L1 (lasso) norm:

Φ(w) = \|w\|

L2-regularized loss function is smooth, meaning the optimum is the stationary point (0-derivative point). L2 can be regarded as weight decay, it won't make the coefficients become zero. On the contrary, L1 can. Meanwhile, L1 regularization gives you sparse estimates.

**Derivative Calculation** 
---
<img src="https://latex.codecogs.com/svg.latex?\frac{dL}{da} = - \frac{y}{a} + \frac{1-y}{1-a}" title="p" />

<img src="https://latex.codecogs.com/svg.latex?dz = \frac{dL(a,y)}{dz} =  (- \frac{y}{a} + \frac{1-y}{1-a})(a(1-a)) = a-y" title="p" />

<img src="https://latex.codecogs.com/svg.latex?dw = \frac{dL}{dw} = x{dz} = \frac {1}{m} \sum^m_{i=1}{x^i}(a^i-y^i)" title="p" />

<img src="https://latex.codecogs.com/svg.latex?db = dz = \frac {1}{m} \sum^m_{i=1}(a^i-y^i)" title="p" />

Experiment: Malicious URL Dection
======
The dataset contains around 420,000 pieces url records. We want to identify if it's a bad url.

Table 1

| url           | label   |    
| ---------        | ------ | 
| iamagameaddict.com     | bad  | 
| slightlyoffcenter.net    | bad   | 

So the 1st step is to cut the url into tokens. As the format of a url is kind of fixed: it contains protocol, host name (primary domain) and so on, we can firstly cut the url by '.', '/', and remove some common parts in the url, such as 'http', 'com', 'net'. Then, we would use logistic regression to do the prediction.

<p float="left"><img src="/images/lg11.png" width="320" /></p>

I have used gridsearchCV to find optimal parameters which is l2, with C (equal to 1/λ) set to be 100, then the classificatio accuracy of training set is around 97%, while for the testing set, it can also be up to 95%.

<p float="left"><img src="/images/lg12.png" width="320" /><img src="/images/lg13.png" width="320" /></p>

References
------
[1]. BISHOP, C. M. (2016). PATTERN RECOGNITION AND MACHINE LEARNING. S.l.: SPRINGER-VERLAG NEW YORK.

[2]. Murphy, K. P. (2013). Machine learning: a probabilistic perspective. Cambridge, Mass.: MIT Press.

[3]. http://blog.fukuball.com/lin-xuan-tian-jiao-shou-ji-qi-xue-xi-ji-shi-machine-learning-foundations-di-shi-jiang-xue-xi-bi-ji/

[4]. https://chenrudan.github.io/blog/2016/01/09/logisticregression.html

[5]. http://www.csuldw.com/2016/09/19/2016-09-19-logistic-regression-theory/

[6]. https://hackernoon.com/gradient-descent-aynk-7cbe95a778da




