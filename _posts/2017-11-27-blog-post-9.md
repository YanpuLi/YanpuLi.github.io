---
title: 'Introduction of Reinforcement Learning'
date: 2017-11-27
permalink: /posts/2017/11/blog-post-9/
tags:
  - reinforcement learning
---

## Introduction
Recently, I have read some papers about Reinforcement Learning (RL). To me, it's quite an interesting and complex topic. So I wrote this post to make a introductary overview of RL. RL is about through interacting with the environment sequentially, the agent will gradually learn how to take actions to maximize an accumulative reward. It acts just like a human being, good behavior results in reward, bad one leads to punishment. This concept is easy to understand. 

But before illustrating the core elements of RL, I'd like to stop here and ask a few questions. We've already got a lot of super powerful "learning", like machine learning, sub-areas: supervised learning, unsupervised learning. What's the relationship between RL and these existing ones? Why RL is still necessary? RL is also a sub-area in machine learning. You can see the difference in the table below.
 
|      | Supervised Learning | Unsupervised Learning    |         RL                     |
|----- |-------------------       | --------------------------| --------------------------------------- |
|**Input** | labeled data             | unlabeled data            | initial state           |
|**Working Process**| train on training set, look for model to optimize certain object function   | look for model to gain insights into data                 | learn from environment in **a sequential process**, take actions, either being punished or rewarded, aiming to maximize reward,             |
|**Validation**  | validate on testing set |             | evaluate by reward function                    |
|**Application** | regression, classification| clustering, segmentation, association| prediction, control|

## Core Elements
A RL system contains 6 core elements: agent, environment, policy, reward signal, value function and model of the environment. In my opinion, the agent and environment is kind like entity, and the rest four elements are like attributes belong to entity. 

On agent side: **Policy** helps agent map environment states to corresponding actions, which can be viewed as a set of responding behaviors in psychology. It can either be deterministic, a = π(s); or stochastic, π(a\|s) = P[A=a\| S=s]. **Reward signal** defines good/ bad events for agent. Thus, if the corresponding reward for an action is low, then the policy may be changed in the next step. **Value function** predicts future rewards. 

On environment side: **Model** is optional in RL system, it could predict what environment will do next, including next reward and next state. So the basic process is from one state to another, the agent choose action based on information (current state) of environment. This action will be emitted and try to change the state of environment. And the environment will in turn outputs a reward to the agent. No one tells the agent what to do, it needs to learn through a a trial-error-correct process.

<p float="left">
  <img src="/images/rl1.png" width="280" />
</p>
Figure 1. Agent & Environment Interactions

## Markov Decision Process
This process can be defined as a Markov Decision Process (MDP). Because each state satisfies markov property: if and only if future state are independent of any previous states or actions. And the environment is fully observable. When the agent observe the environment indirectly, meaning the observation received by the agent is dependent on the current state and the previous action, it becomes a partiarlly observable Markov Decision Process (POMDP). 
{% capture notice-2 %}
Attention: POMDP is still MDP, it can be converted into MDP.
{% endcapture %}
<div class="notice--warning">{{ notice-2 | markdownify }}</div>
A MDP can be represented as a 5-tuple (S, A, P, R, γ). S: set of states, A: set of actions. P: transition probability, R: reward function, γ: a discount factor, ranging from 0 to 1.
{% capture notice-3 %}
Why γ < 1: because it can prevent infinite accumulation of rewards in a non-episodic MDP, where there is no terminal state. While in an episodic MDP, the state will be reset after each episode of a certain length.
If γ close to 0, leads to short-sighted evaluation, otherwise, it will lead to far-sighted evaluation.
{% endcapture %}
<div class="notice--info">{{ notice-3 | markdownify }}</div>
The goal of a RL is to find the optimal policy which maximize the expected return from all states. 

π<sup>* </sup> = argmax<sub>π</sub>E[ R \| π]

To solve the problem, there are 3 main ways: value-based, policy-based and actor-critic (hybrid of the previous two).

**Value functions** method estimates the expected return of being in a given state s. 

The state-value function:

V<sub>π</sub> = E[ G<sub>t</sub> \| S<sub>t</sub> = s]

Where, G<sub>t</sub> denotes the total sum of discounted rewards from time t:

G<sub>t</sub> = R<sub>t+1</sub> + γR<sub>t+2</sub> + ... 
              = ∑<sub>k=0</sub><sup>∞</sup>γ<sup>k</sup> R<sub>t+K+1</sub>

The action-value function:
q<sub>π</sub>(s,a) = E[ G<sub>t</sub>| S<sub>t</sub> = s, A<sub>t</sub> = a]

To solve these equations, we can apply Bellman equation, which help to rewrite the value function in terms of the payoff from one initial state and its successors. So the state-value function can be changed into:

V<sub>π</sub> = E[  R<sub>t+1</sub> + γR<sub>t+2</sub> + ... | S<sub>t</sub> = s]
              = E[  R<sub>t+1</sub> + γG<sub>t+1</sub> | S<sub>t</sub> = s]


Reference
========
[1]. Sutton, R. S. & Barto, A. G. (1998) Reinforcement Learning: An Introduction. Cambridge, Mass.: MIT Press.

[2]. Leslie, P. K. (1996) Reinforcement learning: A survey. Journal of artificial intelligence research 4, 237-285

[3]. https://en.wikipedia.org/wiki/Bellman_equation

[4]. http://www.cs.ubc.ca/~murphyk/Bayes/pomdp.html

[5]. Si, J. (2004) Reinforcement Learning and Its Relationship to Supervised Learning. doi: 10.1002/9780470544785.ch2

[6]. Kai, A. (2017) A Brief Survey of Deep Reinforcement Learning. IEEE Signal Processing Magazine, 34(6), 26-38, arXiv:1406.2040

[7]. Yuxi Li. (2017) Deep Reinforcement Learning: An Overview. arXiv preprint arXiv:1701.07274

https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/
http://www.cs.ubc.ca/~murphyk/Bayes/pomdp.html

------