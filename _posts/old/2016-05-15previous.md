---
title: 'Logistic Regression'
date: 2016-05-15
permalink: /posts/2016/05/blog-post-3/
---
Introduction
======
Logistic regression is to directly estimate the distribution of a conditional probability model from the training set, and obtain the unknown parameters by using maximum likelihood estimation. 

The model can be defined as:
p(Y=1|X) = g(θ<sup>T</sup>X) = 1/(1+ e<sup>-θ<sup>T</sup>X</sup>)

where g(z) = 1/(1+ e<sup>-z</sup>) is the logistic function (sigmoid function), θ = <θ<sub>0</sub>, θ<sub>1</sub>,..., θ<sub>d</sub>>, X = <1, X<sub>1</sub>,..., X<sub>d</sub>>

As g(z) goes towards 1, when z → ∞; towards 0, when z → -∞, shown in Fig. 1
<p float="left"><img src="/images/lg1.png" width="220" /></p>
Figure 1. Sigmoid Function

So our model can be represented as, 
<p float="left"><img src="/images/lg2.png" width="220" /></p>
<p float="left"><img src="/images/lg3.png" width="230" /></p>
<p float="left"><img src="/images/lg4.png" width="380" /></p>
**MLE**
---
Assuming all the training set were generated independently, so the likelihood of the parameters can be represented as:
<p float="left"><img src="/images/lg5.png" width="380" /></p>
Then, we want to obtain θ which maximize the log likelihood: 
<p float="left"><img src="/images/lg6.png" width="380" /></p>
**Gradient Ascent**
---
As the conditional likelihood for logistic regression is concave, we can find optimal θ by using gradient ascent. Repeatedly update the weights in the direction of the gradient:
<p float="left"><img src="/images/lg7.png" width="260" /></p>
<p float="left"><img src="/images/lg8.png" width="260" /></p>
where α is a positive constant learning rate to control the size of each step. As l(θ) is a concave function, the gradient ascent procedure will converge to a global maximum rather than a local minimum.
**Regularization**
---
Regularization is used for reduce overfitting by adding penalty for large values of θ. One way is to add L<sub>2</sub> (ridge) norm to the log likelihood:
<p float="left"><img src="/images/lg9.png" width="280" /></p>
The other is using L<sub>1</sub> (lasso) norm:
<p float="left"><img src="/images/lg10.png" width="280" /></p>

Experiment: Malicious URL Dection
======
The dataset contains around 420,000 pieces url records. We want to identify if it's a bad url.
Table 1

| url           | label   |    
| ---------        | ------ | 
| iamagameaddict.com     | bad  | 
| slightlyoffcenter.net    | bad   | 

So the 1st step is to cut the url into tokens. As the format of a url is kind of fixed: it contains protocol, host name (primary domain) and so on, we can firstly cut the url by '.', '/', and remove some common parts in the url, such as 'http', 'com', 'net'. Then, we would use logistic regression to do the prediction.
<p float="left"><img src="/images/lg11.png" width="280" /></p>
I have used gridsearchCV to find optimal parameters which is l2, with C (equal to 1/λ) set to be 100, then the classificatio accuracy of training set is around 97%, while for the testing set, it can also be up to 95%.
<p float="left"><img src="/images/lg12.png" width="280" /><img src="/images/lg13.png" width="280" /></p>




References
------
[1]. BISHOP, C. M. (2016). PATTERN RECOGNITION AND MACHINE LEARNING. S.l.: SPRINGER-VERLAG NEW YORK.

[2]. Murphy, K. P. (2013). Machine learning: a probabilistic perspective. Cambridge, Mass.: MIT Press.
