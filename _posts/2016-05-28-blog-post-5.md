---
title: 'Logistics Regression VS SVM'
date: 2016-05-28
permalink: /posts/2016/05/blog-post-5/
tags:
  - Logistic regression
  - SVM
  - Classification
---

##Similarities
Both logistic regression and svm is used for classification (svm can also be used for regression); and have linear seperable decision boundary (kernel can also be added in logistic regression). 
Logistic regression and svm are highly related, we can try to change the form to derive svm by using a logistic regression model.
For a two-class problem:
<p float="left"><img src="/images/lgsvm1.png" width="180" /></p>
If we change the probability output of logistic regression into class, by setting a constraint on the likelihood ratio: p(y=1|x)/p(y=0|x) >=c, c>0; take log of both sides: log(p(y=1|x)) -log(p(y=0|x)) >=log(c); put the origin definition back: w<sup>T</sup>x +b >= log(c); we pick c to make log(c) =1, and put a quadratic penalty on the weights, then we can get:
<p float="left"><img src="/images/lgsvm2.png" width="260" /></p>
##Differences
Let's list the loss functions here to tell the differences between LR and svm.
<p float="left"><img src="/images/lg9.png" width="220" /></p>
<p float="left"><img src="/images/svm_formula10.png" width="260" /></p>
Logistic regression tries to maximize the probability of data, and it works on the whole training data. LR wants to make sure the data is as much further away from the decision boundary as possible. 

Svm constructs a hyperplane which is obtained by maximizing the margin. This margin is defined by a small amount of data (support vectors), and these support vectors are the points closest to the decision boundary. 
------
Large number of features and smaller number of training examples: Given that the number of features are relatively larger than the training examples, one could use either of Logistic Regression or SVM without a kernel or Linear Kernel. For instances, lets say the text classification example. In a typical spam email classification example, each of the spam word could become a feature and each of the email could become a training example. Thus, there will be 10000 or more features (words categorized as representative of spam) with 10 â€“ 1000 emails (training examples) to be used for learning algorithm. In examples like these, typically, it is recommended that one should use either logistic regression or SVM without a Kernel or SVM with Linear Kernel as there is not enough training data to fit a complicated non-linear function such as SVM with Gaussian kernel.
Smaller number of features and large, but not very large, number of training examples: In scenarios when the number of features are relatively smaller than the number of training examples, and the number of training examples are not that large, one may use SVM with Gaussian Kernel. In such scenarios, the number of feature set could vary from 1 to 1000 and the number of training examples could vary from 10 to upto 50000 but not bigger than that.
Smaller number of features and very large number of training examples: In scenarios where number of training examples are much much larger than the number of features, one could go on for adding more features and later use SVM without a Kernel (Linear Kernel) or Logistic Regression. In such scenarios, the number of training examples could be as large as 100,000 or may be in millions.