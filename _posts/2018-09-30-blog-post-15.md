---
title: 'Risk Prediction Concept for Medical/Healthcare Data'
date: 2018-09-30
permalink: /posts/2018/09/blog-post-15/
comments: true
tags:
  - Risk Prediction
  - Healthcare
 
---

This post is for explaining some basic statistical concepts used in disease morbidity risk prediction. As being a CS student, I have found a hard time figuring out these statistical concepts, hope my summary would be helpful for you.

### Relative Risk vs Odds Ratio
Before explaining the concepts of these two terms, it is more fundamental to get clear with the following table, which is for calculating the above terms. 'Disease' contains two groups ( e.g. a person with certain disease, like CHD, or without CHD). 'Exposure': whether that person is exposed to certain condition (e.g. somking or no smoking; gender as male or female). <ins> Attention: 'Disease' can be regarded as 'Actual Class' in confusion matrix, but 'Exposure' is not equivalent to the 'Predicted Class'. </ins>
<p float="left">
  <img src="/images/relative risk1.png" width="400" />
</p>
### Figure 1. Disease & Exposure Table

**Relative Risk**: also called as risk ratio, rate ratio, 

**Odds**:

**Odds Ratio**:


### Similarity Control & Knowledge Transfer
**Similarity Control** 

This topic has been mainly discussed in [1] by taking SVM as the example. Vladimir describes the topic as "The mechanism to control students' concept of similarity between training examples". The general idea is: in a seperable sample set, svm only needs to estimate n parameters of w, while in a non-seperable one, it becomes n+l (n parameters of w defines the hyperplane and l values of slacks, which belongs to high VC dimension).

In SVM+, it maps vectors x into space Z and x<sup> * </sup> into Z<sup> * </sup>, which represents the decision space and correcting (slack) space. Vladimir has changed the traditional Lagrangian function (SVM) into:
<p float="left">
  <img src="/images/pi2.png" width="540" />
</p>
subject to constraints:
<p float="left">
  <img src="/images/pi3.png" width="220" />
</p>
The first two items represent the separable part, the rest stands for the correcting space. Note that if α<sub>i</sub> +β<sub>i</sub> −C =0 or  γ tends to 0, then the solution will be back to traditional SVM. This happens when similarity measures in Z<sup>* </sup> are not appropriate.
### Knowledge Transfer
The basic idea is teacher masters rules of mapping X<sup>* </sup> to y, which is much smaller than the rules in original space X. Students need to know how to transfer useful features in X<sup>* </sup> into X. Also, as it only requires a small fraction of the entire knowledge, it doesn't need to work on large-sized sample set.

Reference
========

[1]. Vladimir, V., Akshay, V. A new learning paradigm: Learning using privileged information. Neural Networks 22(2009), 544-557. doi:10.1016/j.neunet.2009.06.042

[2]. Vladimir, V., Rauf, I. Learning Using Privileged Information: Similarity Control and Knowledge Transfer. Journal of Machine Learning Research 16(2015), 2023-2049.




------