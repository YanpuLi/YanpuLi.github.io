---
title: 'Tree Series 2: GBDT, Lightgbm, XGBoost, Catboost'
date: 2018-05-19
permalink: /posts/2018/05/blog-post-13/
comments: true
tags:
  - ML
  - tree
  - Boosting
 
---
## Introduction

Both bagging and boosting are designed to ensemble weak estimators into a stronger one, the difference is: bagging is ensembled by parallel order to decrease variance, boosting is to learn mistakes made in previous round, and try to correct them in new rounds, that means a sequential order. GBDT belongs to the boosting family, with a various of siblings, e.g. adaboost, lightgbm, xgboost, catboost. In this post, I will mainly explain the principles of GBDT, lightgbm, xgboost and catboost, make comparisons and elaborate how to do fine-tuning on these models.

## Gradient Boosting

Beforing elaborate the details, I will leave you 10 minutes to think about a few quick questions of Boosting, which will help you get clear with the procedures of this model. The key of boosting is sequentially learn the "lessons" made in previous round, so:

**1. How to combine previous learners' results with this round?**

**2. In what form to represent and add these previous results?**

**3. GB stands for gradient boosting, what's gradient for in this algorithm?**

**`Ready to go?`**

Gradient Boosting Machine learns the errors made in previous rounds, and tries to correct them. These errors are represented as residuals, to be more precise, it works on fitting **`the gradient of the residuals`**. That's why it's called gradient boosting.
<p float="left"><img src="/images/GBDT2.jpg" width="600" /></p>

**Figure 1. Gradient Boosting Machine Algorithm**

Please forgive my twisted handwriting, writing down the algorithm by hand indeed helps me memorizing the details. In a nutshell, in the kth iteration, the algorithm tries to fit a new single learner h<sub>k</sub> into the last residuals. Shown in Figure 1, it minimizes the mean squared error of the residual and h<sub>k</sub> by finding the optimal w<sup>* </sup>. Then, add h<sub>k</sub> multiply a parameter ρ to F<sub>k-1</sub> to form F<sub>k</sub>. 

**Quick question: why ρ is needed? why not directly add h<sub>k</sub> to F?**

ρ can be viewed as a learning rate(learning step), it's a strategy for avoiding overfitting. By adding a parameter, which is less than 1, though the learning speed will be slowed down, it guarantees the importance of each h<sub>k</sub> won't become too much. Usually, a penalty Ω on tree complexity(a regularization method) will be also added to inhibit overfitting. I will summarize all the strategies for overfitting later.

## GBDT
GBDT has some variation from GBM, e.g. h<sub>k</sub> is referred to DT in GBDT, F<sub>k</sub> is the ensemble of DTs, residual equals to y<sub>i</sub> minus F<sub>k-1</sub>, the searching space is J non-overlapping regions, {R<sub>j</sub>}.
<p float="left"><img src="/images/GBDT3.png" width="600" /></p>

**Figure 2. GBDT Algorithm**

**Table 1. Tree Constraints for overfitting**

| Constraints   | How & Why   | Degree of Control |   
| ---------        | ------ | ------------------|
| # trees     |  the more the better, as it averages the variance  | relatively slow to decrease overfitting|
| Max depth    | shallow trees have less chance to overfit, block bias splitting towards certain direction for each tree  | relatively strong influencer|
| # leaves  | set maximum limitation, there will be less split, means controlling tree complexity  | |
| # instances per split    | if a node's # instances is less than certain threshold, or will be less than that number after split, then forbid this splitting, reason: same as above  | |
| minimum loss improvement    | if improvement < certain threshold, then forbid this splitting: reason: same as above  | |

In Table 1, I have listed the methods which could be used for controlling overfitting. These methods can be divided into 2 groups: one is limit the number of trees, the other is limit the complexity of each single tree. There are some other controlling methods, e.g. sampling, sub-sampling, I will included them later in the section about parameter tuning for specific tree models.

## Finding Best Splitting
We know the best splitting comes with the point which maximize the loss function, but how to find the point? Let me explain this in a reversed way: the searching space for best split is limited, which is a set of proposed splits, and there are two ways to find the proposed split sets, one is exact greedy searching, which is firstly sort all the values of that feature, then calculate the loss reduction for each value. The other is an approximate method, which uses percentile instead. 
<p float="left"><img src="/images/split.png" width="360" /></p>

**Figure 3. Find Best Splits Workflow**
## XGBoost

## Lightgbm

## Catboost


Reference
========

[1].Friedman, J. H. (February 1999). "Greedy Function Approximation: A Gradient Boosting Machine".

[2].http://www.flickering.cn/machine_learning/2016/08/gbdt%E8%AF%A6%E8%A7%A3%E4%B8%8A-%E7%90%86%E8%AE%BA/

[3].https://en.wikipedia.org/wiki/Gradient_boosting

[4].Anna Veronika Dorogush, Vasily Ershov, Andrey Gulin "CatBoost: gradient boosting with categorical features support". Workshop on ML Systems at NIPS 2017.

[5].Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu. "LightGBM: A Highly Efficient Gradient Boosting Decision Tree". Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 3149-3157.

[6].Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. In 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016


------