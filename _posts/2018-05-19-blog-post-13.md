---
title: 'Tree Series 2: GBDT, Lightgbm, XGBoost, Catboost'
date: 2018-05-19
permalink: /posts/2018/05/blog-post-13/
comments: true
tags:
  - ML
  - tree
  - Boosting
 
---
## Introduction

Both bagging and boosting are designed to ensemble weak estimators into a stronger one, the difference is: bagging is ensembled by parallel order to decrease variance, boosting is to learn mistakes made in previous round, and try to correct them in new rounds, that means a sequential order. GBDT belongs to the boosting family, with a various of siblings, e.g. adaboost, lightgbm, xgboost, catboost. In this post, I will mainly explain the principles of GBDT, lightgbm, xgboost and catboost, make comparisons and elaborate how to do fine-tuning on these models.

## GBDT

Beforing elaborate the details, I will leave you 10 minutes to think about a few quick questions of GBDT, which will help better understanding of this model. The key of boosting is sequentially learn the "lessons" made in previous round, so:

**1. How to combine previous tree results with this round?**

**2. In what form to add these previous results?**

**3. GBDT stands for gradient boosting DT, what's gradient used for in this algorithm?**

**Ready to go?**

GBDT learns the errors made in previous rounds, and tries to correct them. These errors are represented as residuals, to be more precise, GBDT works on fitting **`the gradient of the residuals`**. That's why it's called gradient boosting.
```
Algorithm: GBDT     
Input: training set D = {(x<sub>i</sub>, y<sub>i</sub>)}<sub>1</sub><sup>N</sup>
Choose an initial f<sub>0</sub>, let F<sub>0</sub> = f<sub>0</sub>;
For k = 1,2,...,K:
```
<p float="center"><img src="https://latex.codecogs.com/svg.latex?     \hat{y_i} = -\frac  {\partial L(y_i, F_{k-1}(x_i))}  {\partial  F_{k-1}(x_i)},  (i = 1,..., N)" title="p" /></p>
<p float="left"><img src="/images/GBDT1.png" width="400" /></p> 



Reference
========

[1].Friedman, J. H. (February 1999). "Greedy Function Approximation: A Gradient Boosting Machine".

[2].http://www.flickering.cn/machine_learning/2016/08/gbdt%E8%AF%A6%E8%A7%A3%E4%B8%8A-%E7%90%86%E8%AE%BA/

[3].https://en.wikipedia.org/wiki/Gradient_boosting

[4].Anna Veronika Dorogush, Vasily Ershov, Andrey Gulin "CatBoost: gradient boosting with categorical features support". Workshop on ML Systems at NIPS 2017.

[5].Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu. "LightGBM: A Highly Efficient Gradient Boosting Decision Tree". Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 3149-3157.

[6].Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. In 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016


------