---
title: 'Tree Series 2: GBDT, Lightgbm, XGBoost, Catboost'
date: 2018-05-19
permalink: /posts/2018/05/blog-post-13/
comments: true
tags:
  - ML
  - tree
  - Boosting
 
---
## Introduction

Both bagging and boosting are designed to ensemble weak estimators into a stronger one, the difference is: bagging is ensembled by parallel order to decrease variance, boosting is to learn mistakes made in previous round, and try to correct them in new rounds, that means a sequential order. GBDT belongs to the boosting family, with a various of siblings, e.g. adaboost, lightgbm, xgboost, catboost. In this post, I will mainly explain the principles of GBDT, lightgbm, xgboost and catboost, make comparisons and elaborate how to do fine-tuning on these models.

## GBDT

Beforing elaborate the details, I will leave you 10 minutes to think about a few quick questions of GBDT, which will help better understanding of this model. The key of boosting is sequentially learn the "lessons" made in previous round, so:

1. How to combine previous tree results with this round? 

2. In what form to add these previous results? 

3. GBDT stands for gradient boosting DT, what's gradient used for in this algorithm? 

Ready to go? 

GBDT learns the errors made in previous rounds, and tries to correct them. These errors are represented as residuals, to be more precise, GBDT works on fitting **`the gradient of the residuals`**. That's why it's called gradient boosting.



Reference
========

[1].Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. In 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016

[2].http://www.flickering.cn/machine_learning/2016/08/gbdt%E8%AF%A6%E8%A7%A3%E4%B8%8A-%E7%90%86%E8%AE%BA/

[3].Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu. "LightGBM: A Highly Efficient Gradient Boosting Decision Tree". Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 3149-3157.

[4].Anna Veronika Dorogush, Vasily Ershov, Andrey Gulin "CatBoost: gradient boosting with categorical features support". Workshop on ML Systems at NIPS 2017.


------