---
title: 'Tree Series 2: GBDT, Lightgbm, XGBoost, Catboost'
date: 2018-05-19
permalink: /posts/2018/05/blog-post-13/
comments: true
tags:
  - ML
  - tree
  - Boosting
 
---
## Introduction

Both bagging and boosting are designed to ensemble weak estimators into a stronger one, the difference is: bagging is ensembled by parallel order to decrease variance, boosting is to learn mistakes made in previous round, and try to correct them in new rounds, that means a sequential order. GBDT belongs to the boosting family, with a various of siblings, e.g. adaboost, lightgbm, xgboost, catboost. In this post, I will mainly explain the principles of GBDT, lightgbm, xgboost and catboost, make comparisons and elaborate how to do fine-tuning on these models.

## GBDT

Beforing elaborate the details, I will leave you 10 minutes to think about a few quick questions of GBDT, which will help you get clear with the procedures of this model. The key of boosting is sequentially learn the "lessons" made in previous round, so:

**1. How to combine previous tree results with this round?**

**2. In what form to represent and add these previous results?**

**3. GBDT stands for gradient boosting DT, what's gradient for in this algorithm?**

**`Ready to go?`**

GBDT learns the errors made in previous rounds, and tries to correct them. These errors are represented as residuals, to be more precise, GBDT works on fitting **`the gradient of the residuals`**. That's why it's called gradient boosting.
<p float="left"><img src="/images/GBDT2.png" width="600" /></p>

**Figure 1. GBDT Algorithm**

Please forgive my twisted handwriting, writing down the algorithm by hand indeed helps me memorizing the details. In a nutshell, in the kth iteration, GBDT tries to fit a new single tree h<sub>k</sub> into the last residuals. Shown in Figure 1, it minimizes the mean squared error of the residual and h<sub>k</sub> by finding the optimal w<sup>* </sup>. Then, add h<sub>k</sub> multiply a parameter ρ to F<sub>k-1</sub> to form F<sub>k</sub>. 

**Quick question: why ρ is needed? why not directly add h<sub>k</sub> to F?**

ρ can be viewed as a learning rate(learning step), it's a strategy for avoiding overfitting. By adding a parameter, which is less than 1, though the learning speed will be slowed down, it guarantees the importance of each h<sub>k</sub> won't become too large.


Reference
========

[1].Friedman, J. H. (February 1999). "Greedy Function Approximation: A Gradient Boosting Machine".

[2].http://www.flickering.cn/machine_learning/2016/08/gbdt%E8%AF%A6%E8%A7%A3%E4%B8%8A-%E7%90%86%E8%AE%BA/

[3].https://en.wikipedia.org/wiki/Gradient_boosting

[4].Anna Veronika Dorogush, Vasily Ershov, Andrey Gulin "CatBoost: gradient boosting with categorical features support". Workshop on ML Systems at NIPS 2017.

[5].Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu. "LightGBM: A Highly Efficient Gradient Boosting Decision Tree". Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 3149-3157.

[6].Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. In 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016


------