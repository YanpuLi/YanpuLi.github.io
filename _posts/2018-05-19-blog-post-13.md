---
title: 'Tree Series 2: GBDT, Lightgbm, XGBoost, Catboost'
date: 2018-05-19
permalink: /posts/2018/05/blog-post-13/
comments: true
tags:
  - ML
  - tree
  - Boosting
 
---
## Introduction

Both bagging and boosting are designed to ensemble weak estimators into a stronger one, the difference is: bagging is ensembled by parallel order to decrease variance, boosting is to learn mistakes made in previous round, and try to correct them in new rounds, that means a sequential order. GBDT belongs to the boosting family, with a various of siblings, e.g. adaboost, lightgbm, xgboost, catboost. In this post, I will mainly explain the principles of GBDT, lightgbm, xgboost and catboost, make comparisons and elaborate how to do fine-tuning on these models.

## Gradient Boosting

Beforing elaborate the details, I will leave you 10 minutes to think about a few quick questions of Boosting, which will help you get clear with the procedures of this model. The key of boosting is sequentially learn the "lessons" made in previous round, so:

**1. How to combine previous learners' results with this round?**

**2. In what form to represent and add these previous results?**

**3. GB stands for gradient boosting, what's gradient for in this algorithm?**

**`Ready to go?`**

Gradient Boosting Machine learns the errors made in previous rounds, and tries to correct them. These errors are represented as residuals, to be more precise, it works on fitting **`the gradient of the residuals`**. That's why it's called gradient boosting.
<p float="left"><img src="/images/GBDT2.jpg" width="460" /></p>

**Figure 1. Gradient Boosting Machine Algorithm**

Please forgive my twisted handwriting, writing down the algorithm by hand indeed helps me memorizing the details. In a nutshell, in the kth iteration, the algorithm tries to fit a new single learner h<sub>k</sub> into the last residuals. Shown in Figure 1, it minimizes the mean squared error of the residual and h<sub>k</sub> by finding the optimal w<sup>* </sup>. Then, add h<sub>k</sub> multiply a parameter ρ to F<sub>k-1</sub> to form F<sub>k</sub>. 

**Quick question: why ρ is needed? why not directly add h<sub>k</sub> to F?**

ρ can be viewed as a learning rate(learning step), it's a strategy for avoiding overfitting. By adding a parameter, which is less than 1, though the learning speed will be slowed down, it guarantees the importance of each h<sub>k</sub> won't become too much. Usually, a penalty Ω on tree complexity(a regularization method) will be also added to inhibit overfitting. I will summarize all the strategies for overfitting later.

## GBDT
GBDT has some variation from GBM, e.g. h<sub>k</sub> is referred to DT in GBDT, F<sub>k</sub> is the ensemble of DTs, residual equals to y<sub>i</sub> minus F<sub>k-1</sub>, the searching space is J non-overlapping regions, {R<sub>j</sub>}.
<p float="left"><img src="/images/GBDT3.png" width="460" /></p>

**Figure 2. GBDT Algorithm**

**Table 1. Tree Constraints for overfitting**

| Constraints   | How & Why   | Degree of Control |   
| ---------        | ------ | ------------------|
| # trees     |  the more the better, as it averages the variance  | relatively slow to decrease overfitting|
| Max depth    | shallow trees have less chance to overfit, block bias splitting towards certain direction for each tree  | relatively strong influencer|
| # leaves  | set maximum limitation, there will be less split, means controlling tree complexity  | |
| # instances per split    | if a node's # instances is less than certain threshold, or will be less than that number after split, then forbid this splitting, reason: same as above  | |
| minimum loss improvement    | if improvement < certain threshold, then forbid this splitting: reason: same as above  | |

In Table 1, I have listed the methods which could be used for controlling overfitting. These methods can be divided into 2 groups: one is limit the number of trees, the other is limit the complexity of each single tree. There are some other controlling methods, e.g. sampling, sub-sampling, I will included them later in the section about parameter tuning for specific tree models.

## Finding Best Splitting
We know the best splitting comes with the point which maximize the loss function, but how to find the point? Let me explain this in a reversed way: the searching space for best split is limited, which is a set of proposed splits, and there are two ways to find the proposed split sets, one is exact greedy searching, the other is approximate algorithm.
<p float="left"><img src="/images/split.png" width="360" /></p>

**Figure 3. Find Best Splits**

As you can see from Figure 4, exact greedy algorithm goes through all the values in that feature for finding the split, it has an inner loop and an outer loop, so the time complexity would be O(# unique I * m). In order to make this algorithm less time-consuming, the values need to be firstly sorted. which is firstly sort all the values of that feature, then calculate the loss reduction for each value. For approximate method, it utilize percentile of that feature's distribution as proposed points, then cut continuous feature into buckets based on these points, aggregate the statistics, and find the best point according to these information. The time complexity has been reduced to O(m). This algorithm has 2 branches, one is coming up with the proposals at the initial tree construction time, and use these proposals through the whole process. The other is calculate new proposals after each splits. It's not hard to see the first one is more efficient, as it doesn't to calculate proposals all the time. But the other would be more accurate for finding the point, so it's better for growing deeper trees.

<p float="left">
  <img src="/images/exact.jpg" width="260" />
  <img src="/images/appro.jpg" width="320" /> 
</p>

**Figure 4. Splitting Finding Algorithm**

<p float="left"><img src="/images/splitloss.jpg" width="460" /></p>
<p float="left"><img src="/images/splitloss2.jpg" width="460" /></p>

**Figure 5. Approximate Algorithm**

With all these knowledge and fomula in mind, then it's much easier for you to understand the difference and improvement of Lightgbm, XGBoost, and Catboost compared to each other and GBDT.

## XGBoost
We all know or have heard of XGBoost is extremely efficient, that's why it becomes the most popular tools for Kaggle competition. But do you know why XGB can work so powerful and fast?

**1. Reduce time by redesigning data access & storage**

The most time-consuming part in tree learning is to sort the feature values. In order to reduce the time, XGB applies column block and cache-aware access strategies, which greatly reduces the data accessing time.

**1.1 Column Block**
XGB stores the data in a compressed column format in in-memory units, which called block. The feature values will be sorted only once before training, instead of doing sorting in each split finding iterations.

**1.1.1 For exact greedy algorithm:**
1. the **entire** data is stored in **a single block**,
2. **one scan** through the block can get **all the statistics** of the proposed points.

**1.1.2 For approximate algorithm:**
1. data subset of rows stored in multiple blocks, instance indices are stored in blocks too,
2. blocks can be distributed over machine, or even stored on disk,
3. enables a linear scan over the sorted columns, and the statistics collecting for each feature can be implemented **parallelized**,
4. block also supports column subsample, which I will talk later.

<p float="left"><img src="/images/xgb2.png" width="1000" /></p>

**Figure 6. Example for Block Parallel**

**1.2 Cache-aware Access**

The model fetches statistical info by instance row index. However, as the instances are sorted by values, the access to these row indices will be non-continuous. When these statitics doesn't fit into CPU cache, the accessing speed will be slowed down.

**1.2.1 For exact greedy algorithm:**
1. Using a thread **pre-fetches** instances from non-continuous memory into a **continuous** buffer(in-memory & physical continuous),
2. then perform gradient statistics accumulation in the continuous buffer.

**1.2.2 For approximate algorithm:**
1. still use block for pre-fetching, but need to be careful about the size, if too large, the calculated gradient statistics can't fit into CPU cache, otherwise, can't store enough data, not efficient for parallelization.

**2. Reduce time by improving the algorithm: enable automatic sparse data handling**

Sparse data is very common in both classfication and regression problems, when doing split finding, XGB offers a new option to be aware of these sparse data. Shown in Figure 7, when doing value sorting, only non-missing entries will be accessed. `Such strategy helps to reduce the time cost for sorting.` XGB also adds a default direction in each node. when coming across a missing value, the entry will be classified into the default direction. 
<p float="left"><img src="/images/xgb1.jpg" width="320" /></p>

**Figure 7. Sparsity-aware Split Finding Algorithm**

## Lightgbm

## Catboost


Reference
========

[1].Friedman, J. H. (February 1999). "Greedy Function Approximation: A Gradient Boosting Machine".

[2].http://www.flickering.cn/machine_learning/2016/08/gbdt%E8%AF%A6%E8%A7%A3%E4%B8%8A-%E7%90%86%E8%AE%BA/

[3].https://en.wikipedia.org/wiki/Gradient_boosting

[4].Anna Veronika Dorogush, Vasily Ershov, Andrey Gulin "CatBoost: gradient boosting with categorical features support". Workshop on ML Systems at NIPS 2017.

[5].Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, Tie-Yan Liu. "LightGBM: A Highly Efficient Gradient Boosting Decision Tree". Advances in Neural Information Processing Systems 30 (NIPS 2017), pp. 3149-3157.

[6].Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. In 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016


------