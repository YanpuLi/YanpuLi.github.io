---
title: 'Clustering-DBScan'
date: 2017-03-11
permalink: /posts/2017/03/blog-post-8

tags:
  - clustering
  - DBScan
---

## Advantage of Density-based Clustering over Partitioning and Hierarchical Methods
### Disadvantage of Partitioning and Hierarchical Methods
Partitioning algorithm, such as k-means, it requires to <ins>*declare k*</ins> in the first step of clustering. Moreover, it has <ins>*restrictions on the shape of clusters (convex shape)*</ins>, meaning it requires gaussian shape clusters.
{% capture notice-2 %}
**Convex set:** An object is convex if for every pair of points inside the object, every point on the straight line segment is also within the object.
{% endcapture %}
<div class="notice--info">{{ notice-2 | markdownify }}</div>
<p float="left">
	<img src="/images/convex.png" width="100" />
	<img src="/images/space.png" width="50" />
	<img src="/images/nonconvex.png" width="100" />
</p>
Figure 1. Convex Set and Non-convex Set

Example of K-means working on non-convex data: I have faked a banana shape dataset, and used kmeans with 2 clusters and 3 clusters to group the data. Shown in the Fig. 2, which will help you understand why kmeans works badly on non-convex data more easily.
<p float="left">
	<img src="/images/1.png" width="200" />
	<img src="/images/2.png" width="200" />
	<img src="/images/3.png" width="200" />
</p>
Figure 2. Kmeans with Non-convex Data

Hierarchical algorithm: it is <ins>*hard to define termination conditions*</ins>. For example, it is difficult to determine the value of D<sub>min</sub>, which is small enough to seperate all clusters and large enough such that no clusters are being split. 
Here we use agglomerative clustering as example, results shown in Fig. 3.
<p float="left">
	<img src="/images/4.png" width="200" />
</p>
Figure 3. Agglomerative clustering

Compared to these 2 algorithms, DBSCAN (a density-based method) only has *a minimal requirement of input, allows arbitrary cluster shape, and works efficiently on large dataset*.
<p float="left">
	<img src="/images/5.png" width="200" />
</p>
Figure 4. DBSCAN
## Define Density
Since it's a density-based problem, we need to know how to measure the density of points. I will introduce 2 important parameters and a few definitions.
The **Eps**-neighborhood of a point p, known as N<sub>Eps</sub>(p), is defined by N<sub>Eps</sub>(p) = {q ∈ D | dist(p,q) ≤ Eps}
**MinPts**: a minimum number of points.
**Core point**: has at least MinPts neighbor points in an Eps ε of it.
**Border point**: is a point that has fewer neighbors than MinPts within ε , but lies within the ε radius of a core point.
**Noise point**: all other points that are neither core nor border points.
**Directly density-reachable**: p is directly density-reachable from q, if  p ∈ N<sub>Eps</sub>(q) and
 |N<sub>Eps</sub>(q)| ≥ MinPts (core point condition).
**Density-reachable**: p is density-reachable from q, if there is a chain of points, p<sub>1</sub>,...p<sub>n</sub>, p<sub>1</sub> = p, p<sub>n</sub> = q such that p<sub>i+1</sub> is directly density-reachable from p<sub>i</sub>.
**Density connected**: p is density-reachable to q, if there is a point o that both p, q are density-reachable from o wrt. Eps and MinPts.
**Cluster**: Let D be a database of points. A cluster C is a non-empty subset of D satisfying the following conditions:
1) ∀ p, q: if p ∈ C and q is density-reachable from p wrt. Eps and MinPts, then q ∈ C. (Maximality)
2) ∀ p, q ∈ C: p is density-connected to q wrt. EPS and MinPts. (Connectivity)
<p float="left">
	<img src="/images/6.png" width="240" />
</p>
Figure 5. Different Types of Points
{% capture notice-2 %}
Attention: directly density-reachable is symmetric for pairs of core points, but not for (core point, border point) pair.
{% endcapture %}
<div class="notice--warning">{{ notice-2 | markdownify }}</div>

Reference
------
[1]. https://en.wikipedia.org/wiki/Convex_set

[2]. https://pafnuty.wordpress.com/2013/08/14/non-convex-sets-with-k-means-and-hierarchical-clustering/

[2]. Martin Ester et al. (1996). A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise, Kdd 96(34), 226-231. 


