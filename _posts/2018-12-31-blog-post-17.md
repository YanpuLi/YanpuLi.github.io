---
title: 'Quick Notes: Useful Terms & Concepts in NLP'
date: 2018-12-31
permalink: /posts/2018/12/blog-post-17/
comments: true
tags:
  - DL
  - NLP
  - word embedding

---
NLP algorithms are designed to learn from language, which is usually unstructured with arbitrary length. Even worse, different language families follow different rules. Applying different sentense segmentation methods may cause ambiguity. So it is necessary to transform these information into appropriate and computer-readable representation. To enable such transformation, multiple tokenization and embedding strategies have been invented. This post is mainly for giving a brief summary of these terms. (For readers, I assume you have already known some basic concepts, like tokenization, n-gram etc. I will mainly talk about word embedding methods in this blog)

## Bag of Words, BoW
BoW works kind like one hot encoding. It's a very simple way to transfer text into numbers. BoW groups all the unique words in a document as a vocabulary without setting the order of each word (just like put all the words in a bag), then count the occurence of each unique word which belongs to a sentence, generating a vector by using these counts. Each vector has the same length, which equals the size of the vocabulary, 
<p float="left">
	<img src="/images/BoW.png" width="260" />
</p>

**Figure 1. BoW Example**

The problem is, if the vocabulary is getting too large, then the vector may become too sparse (containing too many 0s), due to limited words within each single sentence. One solution is to substitute each word (token) with N-gram. Taking bigram as an example, the first sentence will be tranformed into: {it was, was the, the best, best of, of times}

## Part of Speech, POS & Chunking

Simply transforming text into numbers by counting the occurence is not strong enough to overcome text ambiguity. For example, 'bear' can work both as a noun or verb, but this difference can't be identified by BoW. Part of speech (POS) tagging can classify each word into different groups, e.g. nouns, pronouns, adjectives, verbs, adverbs, prepositions, conjunctions and interjections.POS tells each word's property, but actually word usually function in groups. Chunking solves this by taking POS tagging results as input, then output a set of chunks, which extract related words as phrases, like noun phrase, verb phrase. 

## Word Embedding

No matter how BOW or POS, chunking are designed to help finding a proper way to represent text in numeric format, they all have certain limitations. From my point of view, BOW is kind like tf-idf, a count-based method, despite of that td-idf tells word frequency, BOW only shows word existence. Both of them can't point out whether certain words are closer than others. In this sense, chunking works slightly better than POS, as it shows phrases. But on one hand chunking can't indicate to what extent they are close to the others, on the other hand, it only recognize common phrases. While for embeddings, e.g. CBOW, skip-gram, these two can identify word closeness based on the given text (its context, to be more specific). So even if uncommon or new phrase shows up in the text, its closeness can be recognized. `BTW, word2vec is a very popular word embedding tool provided by Google. The model used in this tool is CBOW & skip-gram. Don't get confused.`

CBOW & Skip-gram have been firstly proposed by Tomas Mikolov in 2013. These embedding methods enable to represent words in a denser-dimension space, and can group similar words. In the following part, I will talk about the principles behind these models and explain why they can caculate similarity among words.

**CBOW & Skip-gram Similarity:** 
- Composition: Both CBOW and skip-gram models divide the text into 2 groups: target word and context (the size of the context depends on the size of the window you set). 
- Input: Words need to be firstly changed into one-hot encoding to pass onto the model. (Previously I got confused here, one-hot is very similar to BoW, but why CBOW can recognize word closeness, BoW can't. Take yr time, I will explain it later.)
- 1st transformation: a V\*N matrix W, V is the vocabulary size(**not context's**), N is a arbitrary number which defines the size of the embedding space. Multiply one-hot word vector with W, get the vector changed into a size N embedding vector. 
- 2nd transformation: a N\*V matrix W', multiply hidden layer vector with W', get a size-V vector.
- Final step before generating output: softmax. (This part is kinda like logit function in Logistic regression. But LR aims to make its output value within [0,1], softmax ensures the sum of each element in the size-V vector equal to 1, so the element with highest probability would most likely be the target word)

<p float="left"><img src="https://latex.codecogs.com/svg.latex?  softmax(z) = \frac  {exp(z_i)}  {\sum_j exp(z_j) }, (j= 1,2,...,V)" title="p" /></p>


**CBOW & Skip-gram Difference:** CBOW predicts which word would be the target word given context, while skip-gram works in an opposite way. The best way to explain something is to show the principle with examples, so I will use one example to go through these two models' working process. 

The corpus is: *I drink coffee everyday.* 

Target: *coffee*

Window Size: *2* 

`as the corpus only contains 4 words, this size-2 window happens to include all the corpus. In the real world problem, the corpus size (V) is usually much larger.`

Context: *{I, drink, everyday}*

### Continuous Bag of Words, CBOW

<p float="left">
	<img src="/images/CBOW.png" width="800" />
</p>

**Figure 2. CBOW Framework with Example**

<p float="left">
	<img src="/images/CBOW2.jpg" width="700" />
</p>


**TBA..**

Reference
========

[1]. https://en.wikipedia.org/wiki/Bag-of-words_model

[2]. https://medium.freecodecamp.org/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24

[3]. Tomas Mikolov, Kai Chen et al. Efficient Estimation of Word Representations in Vector Space

[4]. https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html

[5]. Xin Rong. word2vec Parameter Learning Explained

[6]. Tomas Mikolov, et al. Distributed Representations of Words and Phrases and their Compositionality

[7]. CS224D: Deep Learning for NLP Lecture Notes: Part 1







------