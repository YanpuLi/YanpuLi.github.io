---
title: 'Notes of Recommendation System'
date: 2015-10-12
permalink: /posts/2015/10/blog-post-2/
tags:
  - Recommendation System 
---
When building a recommendation system, it mainly deals with problems, such as: How to collect data (known ratings) in the utility matrix; How to estimate unknown ratings from the known; Evaluating approach 

Dataset
======
There are two types of entities, users and items. A utility matrix is used for storing each user-item pair, and it is usually a sparse matrix. This is because most people haven't rated most items, new items have no ratings, or new users have no history information.  
Based on these features, there are mainly 2 groups of architechtures to build a recommendation system: content-based and collaborative filtering. In this post, I mainly focus on talking about these two approaches. 

Content-based
======
  * Main idea: focus on item properties, recommend items to customer x, which is similar to previous items rated highly by x
  * Example: recommend movies with same actors

**Item Profile**
In content-based system, an item profile must be created for each item, recording important properties of that item. For text content, we can use TF-IDF to calculate scores for the document to create doc profile, and pick the words with highest scores as important features. w<sub>j</sub> = (w<sub>1j</sub>,...,w<sub>ij</sub>,...,w<sub>kj</sub>)

For images, one way is to let the users making tags for each pictures. However, users may not be willing to do this, as it is very troublesome.

**User Profile**
It is used for describing the users' preferences. It can be created into 2 ways. 
* weighted average of rated item profiles
* variations: weight by difference from average rating for item. Given user profile w<sub>x</sub>, item profile w<sub>j</sub>:       
r<sub>xj</sub> = cos(w<sub>x</sub>, w<sub>j</sub>) = w<sub>x</sub>w<sub>j</sub> / ||w<sub>x</sub>||.||w<sub>j</sub>||
<p float="left"><img src="/images/rs1.png" width="180" /></p>
**Pros & Cons for Content-base System**

| Pros                                | Cons   |                                                              
| --------                            | ------ | 
| no need for data on other users     | hard to choose proper features   |                       
| able to recommend to users with unique tastes    | hard to create user profile for new users   |                         
| able to recommend new unpopular items     | never recommends items beyond user's content profile   |                         
| can provide explanations (item features)   | unable to exploit quality judgements of other users   |                         

Collaborative filtering
======
  * Main idea: focus on harnessing quality judgements of other users, it includes 2 process, finding users having similar tastes, and recommending what the similar users like.

**Find Similar Users:** r<sub>x</sub>: the vector of user x's ratings
* Jaccard distance: ignore ratings, only focus on which items have been rated, 
* Cosine distance: problem: it will tread missing ratings as negative. 
Sim(x,y) = cos(r<sub>x</sub>, r<sub>y</sub>) = r<sub>x</sub>r<sub>y</sub>||r<sub>x</sub>||.||r<sub>y</sub>||
* Pearson correlation coefficient:

Sim(x,y) = cos(r<sub>x</sub>, r<sub>y</sub>) 
         = (r<sub>x</sub>-r<sub>x,avg</sub>)(r<sub>y</sub>-r<sub>y,avg</sub>)||r<sub>x</sub>-r<sub>x,avg</sub>||.||r<sub>y</sub>-r<sub>y,avg</sub>||

**Example:**
<p float="left"><img src="/images/rs2.png" width="300" /></p>
Figure 1. Utility Matrix

| Method           | (A,B)  | (A,C)  |
| -----------------| ------ | -------|
| [Jaccard](#)     | 1/5    | 2/4    |
| [Cosine](#)      | 0.38   | 0.322  |
| [Pearson](#)     | 0.092  | -0.559 |


Intuitively, we want sim(A,B) > sim(A,C). Because both A,B have a high rating for HP1, and A,C have two polar ratings for TW and SW1. However, by using Jaccard distance, it returns the values showing C is more close to A, rather than B. This tells us that Jaccard is less suitable for the case with detailed ratings. Instead, it only cares if the item has been rated. Cosine distance is kind of better than Jaccard. But it still can't significantly tell the difference between (A,B) and (A,C). This may be caused by the 'negative' missing ratings.
Finally, by normalizing ratings with average scores, Pearson coefficient has remarkably tells these two groups apart.

**User-User Collaborative Filtering:**
For user u, find similar users, estimate rating for item i based on ratings from similar users
<p float="left"><img src="/images/rs3.png" width="300" /></p>

**Item-Item Collaborative Filtering:**
For item i, find other similar items, estimate rating for i based on similar items. S<sub>ij</sub> denotes similarity between item i and j, r<sub>xj</sub> denotes ratings of user x on item j, N(i;x) represents set of items rated by user x, similar to item i
<p float="left"><img src="/images/rs4.png" width="240" /></p>

**Example:**
<p float="left"><img src="/images/rs5.png" width="350" /></p>























Headings are cool
======

You can have many headings
======

Aren't headings cool?
------