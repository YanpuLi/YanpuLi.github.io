---
title: 'DL Series1: Sequence Neural Network'
date: 2018-07-05
permalink: /posts/2018/07/blog-post-14/
comments: true
tags:
  - DL
  - RNN
 
---
## Introduction

Finally, I'm writing something about neural network. I will start with the branch I'm most familiar with, the sequential neural network. In this post, I won't talk about the forward/ backword propogation, as there are plenty of excellent blogs and online courses. My motivation is to give clear comparison between RNN, LSTM and GRU. Because I find it's very important to bare in mind the **structural differences** and the **cause and effect between model structure and function** of these models. These knowledge help me understand why certain type of model works well on certain kinds of problems. Then when working on real world problems(e.g. time series, name entity recognition), I become more confident in choosing the most appropriate model.

## RNN and Traditional ANN

No matter how deep or complex a neural network is, it is typically composed of 3 parts: an input layer, several hidden layers(1 or more) and one output layer. Within each layer, it tries to transform the input based on certain function(softmax, relu, logit) and selectively passes(dropout) the data onto the next layer. So, why NN is so powerful? In my opinion, the key is data representation. With each layer's transformation, ANN enables data to be represented in a more and more appropriate way. Then, useful information can be more easily to be extracted out. 

<p float="left">
	<img src="/images/RNN1.png" width="300" />
	<img src="/images/RNN2.png" width="300" />
</p>

**Figure 1. Deep Feed Forward NN and RNN**<sub>from Fjodor van Veen - asimovinstitute.org</sub>



Reference
========

[1].Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. In 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016




------