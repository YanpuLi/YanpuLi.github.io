---
title: 'DL Series1: Sequence Neural Network and Its Variants'
date: 2018-07-05
permalink: /posts/2018/07/blog-post-14/
comments: true
tags:
  - DL
  - RNN
  - LSTM
  - GRU

---
## Introduction

Finally, I'm writing something about neural network. I will start with the branch I'm most familiar with, the sequential neural network. In this post, I won't talk about the forward/ backword propogation, as there are plenty of excellent blogs and online courses. My motivation is to give clear comparison between RNN, LSTM and GRU. Because I find it's very important to bare in mind the **structural differences** and the **cause and effect between model structure and function** of these models. These knowledge help me understand why certain type of model works well on certain kinds of problems. Then when working on real world problems(e.g. time series, name entity recognition), I become more confident in choosing the most appropriate model.

## RNN and Traditional ANN

No matter how deep or complex a neural network is, it is typically composed of 3 parts: an input layer, several hidden layers(1 or more) and one output layer. Within each layer, it tries to transform the input based on certain function(softmax, relu, logit) and selectively passes(dropout) the data onto the next layer. So, why NN is so powerful? In my opinion, the key is about data representation. With each layer's transformation, ANN enables data to be represented in a more and more appropriate way. Then, useful information can be more easily to be extracted out. The left section in Figure 1, shows a three layer feed forward NN. Every node(except the output layer) in the network has been connected to all the nodes in the direct following layer. No connections are made within a single layer, meaning all nodes from one layer are **independent.**

For sequential NN, e.g. RNN, connections are existed both within layers and among layers(to be specific: the layer refers to the hidden layer). So why links within layers are needed? Because such links support information to be passed from node to node within layers. Then, why do we need to pass information sequentially in the same layer? Because when working on senarios, like language translation, context info is extremely important for the final translation. ('Patient A denied that he had diabetes problems.')

<p float="left">
	<img src="/images/RNN1.png" width="260" />
	<img src="/images/RNN2.png" width="320" />
</p>

**Figure 1. Deep Feed Forward NN and RNN**<sub>source: Fjodor van Veen - asimovinstitute.org</sub>



Reference
========

[1].Tianqi Chen and Carlos Guestrin. XGBoost: A Scalable Tree Boosting System. In 22nd SIGKDD Conference on Knowledge Discovery and Data Mining, 2016




------