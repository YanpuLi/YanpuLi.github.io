---
title: 'DL Series1: Sequence Neural Network and Its Variants'
date: 2018-07-05
permalink: /posts/2018/07/blog-post-14/
comments: true
tags:
  - DL
  - RNN
  - LSTM
  - GRU

---

## Introduction
Finally, I'm writing something about neural network. I will start with the branch I'm most familiar with, the sequential neural network. In this post, I won't talk about the forward/ backword propogation, as there are plenty of excellent blogs and online courses. My motivation is to give clear comparison between RNN, LSTM and GRU. Because I find it's very important to bare in mind the **structural differences** and the **cause and effect between model structure and function** of these models. These knowledge help me understand why certain type of model works well on certain kinds of problems. Then when working on real world problems(e.g. time series, name entity recognition), I become more confident in choosing the most appropriate model.

## RNN and Traditional ANN
No matter how deep or complex a neural network is, it is typically composed of 3 parts: an input layer, several hidden layers(1 or more) and one output layer. Within each layer, it tries to transform the input based on certain function(softmax, relu, logit) and selectively passes(dropout) the data onto the next layer. So, why NN is so powerful? In my opinion, the key is about data representation. With each layer's transformation, ANN enables data to be represented in a more and more appropriate way. Then, useful information can be more easily to be extracted out. The left section in Figure 1, shows a three layer feed forward NN. Every node(except the output layer) in the network has been connected to all the nodes in the direct following layer. No connections are made within a single layer, meaning all nodes from one layer are **independent.**

For sequential NN, e.g. RNN, connections are existed both within layers and among layers(to be specific: the layer refers to the hidden layer). So why links within layers are needed? Because such links support information to be passed from node to node within layers. Then, why do we need to pass information sequentially in the same layer? Because when working on senarios, like language translation, context info is extremely important for the final translation. For example: 'Patient A denied that he had diabetes problems.' A feed forward NN would pass each word into a unit, and process the word independently from one layer to another. So the final output may be identified the patient had diabetes. 

<p float="left">
	<img src="/images/RNN1.png" width="260" />
	<img src="/images/RNN2.png" width="320" />
</p>

**Figure 1. Deep Feed Forward NN and RNN**<sub>  Source: Fjodor van Veen - asimovinstitute.org</sub>

The intrinsic property of RNN is its hidden unit can capture previous info as memories. And this can be implemented in two different ways. One way is shown in the left part in Figure 2. Memories of previous info would be passed through the loop in that graph, and the black sqaure indicates one time step delay interaction. The other way is to unfolding the operation by using multiple copies of a single unit, and the number of these copied units equals to the size of the input sequence. The first one is very succinct, while the second one allows the model use **same transition functions and share parameters at every time step,** which decreases the # of parameters a model need to learn.

<p float="left">
	<img src="/images/RNN4.png" width="100" />
	<img src="/images/RNN5.png" width="300" />
</p>

**Figure 2. Folded and Unfolded RNN Hidden Unit Structure**<sub>  Source: Nature</sub>

## RNN Application Scenarios
So what is RNN capable of? What kind of scenarios can RNN be applied to? The answer is any scenes which are in great need of context information, e.g. NLP and Time Series tasks. To be more specific, the detailed structures of RNN can be customerized into different types, according to each task's input and output requirements. For example, to summarize the content shown in a single image, a one-to-many structure would be a proper answer. For language translation, each word input needs to be translated. So the optimal structure would be many-to-many.

<p float="left">
	<img src="/images/RNN6.png" width="700" />
</p>

**Figure 3. Different Types of RNN and Related Application Scenarios**

{% capture notice-2 %}
Attention: Though RNN is designed to carry context info, actually it can only look back within limited steps. This is caused by vanishing gradients. As the info passes along the units, it would be multipled by a small number(<0) over and over again, and eventually become lost. 
{% endcapture %}
<div class="notice--warning">{{ notice-2 | markdownify }}</div>


Reference
========

[1].http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/

[2].https://colah.github.io/posts/2015-09-NN-Types-FP/

[3].https://en.wikipedia.org/wiki/Recurrent_neural_network

[4].https://colah.github.io/posts/2015-08-Understanding-LSTMs/

------