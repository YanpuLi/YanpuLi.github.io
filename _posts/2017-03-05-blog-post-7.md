---
title: 'Clustering-Kmeans'
date: 2017-03-05
permalink: /posts/2017/03/blog-post-7
tags:
  - clustering
  - kmeans
---
## Introduction of Clustering

Clustering is a process of grouping similar objects together. It belongs to unsupervised learning, as it's unlabeled. There is a set of different clustering method, including partitioning method (flat clustering), hierarchical clustering and density-based method.

To measure the similarity between two objects, we can use different criteria corresponding to each specific problem. 

If the object is represented as a vector: measure similarity by cosine similarity, ranging from -1 to 1.
<p float="left"><img src="/images/cosine.png" width="260" /></p>
To keep a positive distance, we can use cosine distance: Dist(A,B) = 1- sim(A,B), so the range will be [0, 2]. So if two objects are getting close, its cosine distance will be close to 0. Otherwise, it will gradually increase to 2. 
{% capture notice-2 %}
Attention: Cosine distance doesn't meet triangle inequality. Suppose A = (1,0), B = (0,1), C = (1,1), we will get dist(A,C) + dist(B,C) < dist(A,B).
To fix this, we can convert it to angular distance, which you can find in reference [1].
{% endcapture %}
<div class="notice--warning">{{ notice-2 | markdownify }}</div>
If the objects are sets: measure similarity by Jaccard Distance.
<p float="left"><img src="/images/Jaccard.png" width="290" /></p>
If the objects are points: measure similarity by Euclidean Distance.
dist(x,y)<sup>2</sup> =∑<sub>j=1</sub><sup>n</sup>(x<sub>j</sub> −y<sub>j</sub>)
## K-means
In this post, we mainly talk about k-means clustering, which is about iteratively assigning and recalculating the closest points to corresponding centroid until there is no difference between two iterations. It mainly contains 4 steps:
1. randomly pick K points from your sample data as initial centroids (μ<sub>k</sub>)
2. assign each sample point to the nearest centroid
3. recalculate the centroid in each new cluster
4. repeat step 2 and 3 until cluster assignment do not change or a certain user-defined requirements have been reached
\
Thus, we can define the object function as: J = ∑<sub>i=1</sub><sup>N</sup>∑<sub>k=1</sub><sup>K</sup>r<sub>ik</sub>||x<sub>i</sub> −μ<sub>k</sub>||<sup>2</sup>. r<sub>ik</sub> means if x<sub>i</sub> has been assigned to cluster k, then it equals 1; otherwise, it equals 0. It's very straight forward. In each cluster, we want to minimize the sum of the squared distance of each data to the centroid. In order to obtain the smallest J, in each iteration, we will have 2 steps. One is keeping μ<sub>k</sub> fixed, to minimize J with respect to r<sub>ik</sub>; the other is making r<sub>ik</sub> fixed, and minimize J with respect to μ<sub>k</sub>. This two-step of updating μ<sub>k</sub> and r<sub>ik</sub> correspond to the expectation and maximization process of EM algorithm, which we will talk in later posts.

## Key Problems
There are three key points of this algorithm:
* how to decide the size of K
* which K points should be chosen initially?
* when to stop the iterations


Reference
======
[1]. https://en.wikipedia.org/wiki/Cosine_similarity

[2]. https://en.wikipedia.org/wiki/Jaccard_index

------



