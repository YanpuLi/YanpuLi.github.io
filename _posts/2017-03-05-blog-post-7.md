---
title: 'Clustering-Kmeans'
date: 2017-03-05
permalink: /posts/2017/03/blog-post-7
tags:
  - clustering
  - kmeans
---

Introduction of Clustering
======
Clustering is a process of grouping similar objects together. It belongs to unsupervised learning, as it's unlabeled. There is a set of different clustering method, including partitioning method (flat clustering), hierarchical clustering and density-based method.

To measure the similarity between two objects, we can use different criteria corresponding to each specific problem. 
* If the object is represented as a vector: measure similarity by cosine similarity, ranging from -1 to 1.
<p float="left"><img src="/images/cosine.png" width="180" /></p>
To keep a positive distance, we can use cosine distance: Dist(A,B) = 1- sim(A,B), so the range will be [0, 2]. So if two objects are getting close, its cosine distance will be close to 0. Otherwise, it will gradually increase to 2. 
{% capture notice-2 %}
Attention: Cosine distance doesn't meet triangle inequality. Suppose A = (1,0), B = (0,1), C = (1,1), we will get dist(A,C) + dist(B,C) < dist(A,B).
To fix this, we can convert it to angular distance.
{% endcapture %}
<div class="notice--warning">{{ notice-2 | markdownify }}</div>
* If the objects are sets: measure similarity by Jaccard Distance.
<p float="left"><img src="/images/Jaccard.png" width="220" /></p>
* If the objects are points: measure similarity by Euclidean Distance.
dist(x,y)<sup>2</sup> =∑<sub>j=1</sub>(x<sub>j</sub> −y<sub>j</sub>)

Reference
======
[1]. https://en.wikipedia.org/wiki/Cosine_similarity

[2]. https://en.wikipedia.org/wiki/Jaccard_index

Aren't headings cool?
------
<p float="left"><img src="/images/cat.png" width="240" /></p>


