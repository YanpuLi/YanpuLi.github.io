---
title: 'Quick Notes: Summarization about Current NER Methods in Deep Learning'
date: 2018-08-26
permalink: /posts/2018/08/blog-post-16/
comments: true
tags:
  - DL
  - NLP
  - NER
  - LSTM
  - CRF

---
Recently, I have been working on NER projects. As a greener, I have spent a lot of time in doing research of current NER methods, and made a summarization. In this post, I will list my summaries(NER in DL), hope this could be helpful for the readers who are interested and also new in this area. Before reading, I assume you already know some basic concepts(e.g. sequential neural network, POSï¼ŒIOB tagging, word embedding, conditional random field).

## Introduction
NER stands for name entity recognition, which aims to find the crutial objects(the entities) from text. Traditional methods are usuallt statistical models, e.g. conditional random field(CRF), hidden markov model(HMM), which requires heavily on feature engineering, and very domain-specific, which means these traditional models need large amount of hand-crafted features and may not be able to fit in new domain. 

With the revival of deep learning models, now the popular methods in NER becomes DL(CNN/RNN/LSTM/GRU)+CRF/HMM. Because DL requires no feature engineering, have long dependency(sequential NN). Moreover, DL models are actually a pretty useful tool for data representation. So data would be first passed to DL, and then use the output of DL as 'features' to train the traditional statistical model. In this sense, the model depends largely on how well the DL layers can represent the data. To solve this problem, researchers utilized different embedding methods(word level/ char level, or add pretrained embedding).

## Customized Input + DL + CRF
**Paper: End to End Learning of Semantic Role Labeling Using Recurrent Neural Networks** 
In this paper, the author designed an end-to-end role labeling framework, by utilizing customized word input format, using 4 bi-directional LSTM to generate data representation, then using CRF as final labeling layer.

**Table 1. Model Structure**

|Input|Processing Level |DL Layers| ML layers|
|-----|-----------------|---------|----------|
|raw text, no manual labeling, using pretrained word representation, with customized input format| word level| 4* Bi-directional LSTM | CRF|

**About Customized word input:**

<p float="left">
	<img src="/images/NER1.png" width="320" />
</p>

**Figure 1. Customized Text Input Format**



{% capture notice-2 %}

{% endcapture %}
<div class="notice--warning">{{ notice-2 | markdownify }}</div>



<a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/"> Christopher Olah</a>

## Summary

LSTM CRF, manual labeling, different fields, more professional words.

Reference
========

[1].http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/





------