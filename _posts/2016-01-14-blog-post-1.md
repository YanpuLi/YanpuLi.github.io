---
title: 'Ensemble learning: Stacking '
date: 2016-01-14
permalink: /posts/2016/01/blog-post-1/
layout: archive
tags:
  - stacking
  - ensemble learning
  - machine learning
---

## Introduction of Stacking

Stacked generalization, stacking, is composed of two types of layers: 
1. base classifiers (generally different learning algorithms)
2. meta learner (tries to combine the output of the first layer to obtain the final results)

To maintain the performance of a stacking model, it requires diversity and accuracy among each classifiers. Because by applying similar models, it is highly possible that they would make wrong decisions at the same instance. However, if the classifiers are diverse, their errors will not be highly correlated, and the combination of classifer will perform better than the base classifiers.

Before explaining stacking algorithm in details, I'd like to make a horizontal comparison of stacking against bagging and boosting. As these three algorithms all belong to ensemble learning, which is to combine a set of classifiers together with their outputs such that the result outperforms all the individual classifiers. Here, I have listed the differences in the table below. 

#### Table 1

| Method       | Goal             |  Data Partition                                        | Classifiers                            | Combination of base models|
| ---------    | ---------------  | ---------------------------------------                | ----------------------------           | ------------|
| Bagging | decrease variance| sampling with replacement                              | same type & parallel ensemble           | average|    
| Boosting | decrease bias    | emphasize training instances, previously mis-classified| same type & sequential ensemble         | weighted majority vote |
| Stacking | both             | CV (generally LOOCV)                                   | diverse types & two layers (base, meta) | meta learner|                            

## Stacking Algorithm

Given dataset D, T base learners, firstly partition into K equal-size subset datasets, similar to K-fold CV process, then,
* Step 1: Prepare input, learn base classifiers
  * input: for each base learner, use **(D - D<sub>i</sub>)** as training set and **D<sub>i</sub>** as testing set
  * base model generation
* Step 2: Constructing training set for meta learner, learn meta model
  * creating training data over D<sub>i</sub>: the output predicted labels from level-0 are kept as new features; thus, at the end of this stage, <ins>it will contain T attributes</ins>, and keep the original class labels as labels in new data
  * meta model generation
* Step 3: Regenerate base learners on the whole dataset D
  * for each learner, it has previously worked on all K subsets in Level-0, generating K fits; <ins>so D will be runned on these K fits, and finally average the results</ins>, this is the test data for meta learner (it is expected that the classifier will be slightly more accurate)

Details are shown in Fig. 1 and Fig. 2.

![GitHub Logo](/images/stacking_algo.png)
### Figure 1. Stacking Algorithm


![GitHub Logo](/images/stacking.jpg)
### Figure 2. Partial Framework

---

<var>**Attention** In Fig. 1, the algorithm works on the wholde dataset to regenerate base models. Alternatively, you can try to split D into training and testing set at the very beginning. Conduct step 1 and step 2 on training part, then regenerate base models only on testing set. One advantage is it can be less time consuming, as it doesn't need to work on the full data. Fig. 2 is a good example to directly show how stacking works. However, in the upper blue section (Training Data), it's a little bit confusing. You may mistakenly think that 5 models seperately work on 1 fold (Predict) and 4 folds (Learn) only once, which is Model 1 predicts on the 5th fold, Model 2 predicts on the 4th fold and so on so forth. In fact, take Model 1 for example, it need to do prediction on all each single fold, and learn from every four folds from the full data.</var>

---
<p align="left">
  <img src="/images/stacking_Implementation.png" width="550"/>
</p>
### Figure 3. Stacking Implementation

## Experiment
dataset: iris data; 
base learner: SVM, Random Forest, KNN;
meta learner: Logistic regression; Fig. 4 shows classification report, confusion matrix for training set, and accuracy for testing set. For each individual base classifier, SVM performs better than the other three. Then, I use SVM as meta learner, the other three as base ones. The testing accuracy is 0.977, which is better than all these 4 individual learners. <var>The reason behind this is in Fig. 4, we can tell that random forest makes some mistakes on class3 instances, logistic regression has other mistakes on class2 and 3 and knn has no mistakes(remember these mistake instances are from training set). So when combining them together, certain mistakes made by one classifer can be counteracted by the correct identification from other classifiers. </var>

<p float="left">
  <img src="/images/individual1.png" width="300" />
  <img src="/images/individual2.png" width="300" /> 
</p>

### Figure 4. Single model performance



Questions about Stacking
========

Though it is not hard to understand the principle of a stacking model, there are still a bunch of questions need to be answered.
* Base Learner
  * which models to use 
  * how many models 
  * what kind of parameters   
* Meta Learner
  * which model to use 
  * what kind of parameters 
  * number of attributes    












Reference
========

[1]. Wolpert, D. H. (1992). Stacked generalization. Neural Networks, 5(2), 241-259. doi:10.1016/s0893-6080(05)80023-1

[2]. Sesmero, M. P., Ledezma, A. I., & Sanchis, A. (2015). Generating ensembles of heterogeneous classifiers using Stacked Generalization. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 5(1), 21-34. doi:10.1002/widm.1143

[3]. Ledezma, A., Aler, R., & Borrajo, D. (n.d.). Heuristic Search-Based Stacking of Classifiers. Heuristic and Optimization for Knowledge Discovery, 54-67. doi:10.4018/978-1-930708-26-6.ch004

[4]. Aggarwal, C. C. (2015). Data classification: algorithms and applications. Boca Raton: CRC Press.

[5]. http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/


------