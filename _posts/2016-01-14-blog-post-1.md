---
title: 'Ensemble learning: Stacking '
date: 2016-01-14
permalink: /posts/2016/01/blog-post-1/
layout: archive
tags:
  - stacking
  - ensemble learning
  - machine learning
---

## Introduction of Stacking

Stacked generalization, stacking, is composed of two types of layers: 
1. base classifiers (generally different learning algorithms)
2. meta learner (tries to combine the output of the first layer to obtain the final results)

To maintain the performance of a stacking model, it requires diversity and accuracy among each classifiers. Because by applying similar models, it is highly possible that they would make wrong decisions at the same instance. However, if the classifiers are diverse, their errors will not be highly correlated, and the combination of classifer will perform better than the base classifiers.

Before explaining stacking algorithm in details, I'd like to make a horizontal comparison of stacking against bagging and boosting. As these three algorithms all belong to ensemble learning, which is to combine a set of classifiers together with their outputs such that the result outperforms all the individual classifiers. Here, I have listed the differences in the table below. 

#### Table 1

| Method       | Goal             |  Data Partition                                        | Classifiers                            | Combination of base models|
| ---------    | ---------------  | ---------------------------------------                | ----------------------------           | ------------|
| Bagging | decrease variance| sampling with replacement                              | same type & parallel ensemble           | average|    
| Boosting | decrease bias    | emphasize training instances, previously mis-classified| same type & sequential ensemble         | weighted majority vote |
| Stacking | both             | CV (generally LOOCV)                                   | diverse types & two layers (base, meta) | meta learner|                            

## Stacking Algorithm

Given dataset D, T base learners, firstly partition into K equal-size subset datasets, similar to K-fold CV process:
* Level-0: generation of base models
  * For each base learner, use (D - ith subset) as training set and ith subset as testing set;
  * Constructing training set for meta learner over the instances in each ith subset: the output predicted labels of the ith subset from level-0 are kept as new features, at the end of this stage, this training set contains T attributes, and use the original class labels;
* Level-1: 
![GitHub Logo](/images/stacking_algo.png)
#### Figure 1. Stacking Algorithm


![GitHub Logo](/images/stacking.jpg)
#### Figure 2. Partial Framework






























Reference
========
Reference
======
[1]. Wolpert, D. H. (1992). Stacked generalization. Neural Networks, 5(2), 241-259. doi:10.1016/s0893-6080(05)80023-1

[2]. Sesmero, M. P., Ledezma, A. I., & Sanchis, A. (2015). Generating ensembles of heterogeneous classifiers using Stacked Generalization. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 5(1), 21-34. doi:10.1002/widm.1143

[3]. Ledezma, A., Aler, R., & Borrajo, D. (n.d.). Heuristic Search-Based Stacking of Classifiers. Heuristic and Optimization for Knowledge Discovery, 54-67. doi:10.4018/978-1-930708-26-6.ch004

[4]. Aggarwal, C. C. (2015). Data classification: algorithms and applications. Boca Raton: CRC Press.


------