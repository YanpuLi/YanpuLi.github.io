---
title: 'Ensemble learning: Stacking '
date: 2016-01-14
permalink: /posts/2016/01/blog-post-1/
layout: archive
tags:
  - stacking
  - ensemble learning
  - machine learning
---

## Introduction of Stacking

Stacked generalization, stacking, is composed of two types of layers: 
1. base classifiers (generally different learning algorithms)
2. meta learner (tries to combine the output of the first layer to obtain the final results)

To maintain the performance of a stacking model, it requires diversity and accuracy among each classifiers. Because by applying similar models, it is highly possible that they would make wrong decisions at the same instance. However, if the classifiers are diverse, their errors will not be highly correlated, and the combination of classifer will perform better than the base classifiers.

Before explaining stacking algorithm in details, I'd like to make a horizontal comparison of stacking against bagging and boosting. As these three algorithms all belong to ensemble learning, which is to combine a set of classifiers together with their outputs such that the result outperforms all the individual classifiers. Here, I have listed the differences in the table below. 

#### Table 1

| Method       | Goal             |  Data Partition                                        | Classifiers                            | Combination of base models|
| ---------    | ---------------  | ---------------------------------------                | ----------------------------           | ------------|
| Bagging | decrease variance| sampling with replacement                              | same type & parallel ensemble           | average|    
| Boosting | decrease bias    | emphasize training instances, previously mis-classified| same type & sequential ensemble         | weighted majority vote |
| Stacking | both             | CV (generally LOOCV)                                   | diverse types & two layers (base, meta) | meta learner|                            

## Stacking Algorithm

Given dataset D, T base learners, firstly partition into K equal-size subset datasets, similar to K-fold CV process, then,
* Step 1: Prepare input, learn base classifiers
  * input: for each base learner, use **(D - D<sub>i</sub>)** as training set and **D<sub>i</sub>** as testing set
  * base model generation
* Step 2: Constructing training set for meta learner, learn meta model
  * creating training data over D<sub>i</sub>: the output predicted labels from level-0 are kept as new features; thus, at the end of this stage, <ins>it will contain T attributes</ins>, and keep the original class labels as labels in new data
  * meta model generation
* Step 3: Regenerate base learners on the whole dataset D
  * for each learner, it has previously worked on all K subsets in Level-0, generating K fits; <ins>so D will be runned on these K fits, and finally average the results</ins>, this is the test data for meta learner (it is expected that the classifier will be slightly more accurate)

Details are shown in Fig. 1 and Fig. 2.

![GitHub Logo](/images/stacking_algo.png)
### Figure 1. Stacking Algorithm


![GitHub Logo](/images/stacking.jpg)
### Figure 2. Partial Framework

---

<var>**Attention:** In Fig. 1, the algorithm works on the wholde dataset to regenerate base models. Alternatively, you can try to split D into training and testing set at the very beginning. Conduct step 1 and step 2 on training part, then regenerate base models only on testing set. One advantage is it can be less time consuming, as it doesn't need to work on the full data. Fig. 2 is a good example to directly show how stacking works. However, in the upper blue section (Training Data), it's a little bit confusing. You may mistakenly think that 5 models seperately work on 1 fold (Predict) and 4 folds (Learn) only once, which is Model 1 predicts on the 5th fold, Model 2 predicts on the 4th fold and so on so forth. In fact, take Model 1 for example, it need to do prediction on all each single fold, and learn from every four folds from the full data.</var>

---

##header









#ssss



Refe
========











Reference
========

[1]. Wolpert, D. H. (1992). Stacked generalization. Neural Networks, 5(2), 241-259. doi:10.1016/s0893-6080(05)80023-1

[2]. Sesmero, M. P., Ledezma, A. I., & Sanchis, A. (2015). Generating ensembles of heterogeneous classifiers using Stacked Generalization. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 5(1), 21-34. doi:10.1002/widm.1143

[3]. Ledezma, A., Aler, R., & Borrajo, D. (n.d.). Heuristic Search-Based Stacking of Classifiers. Heuristic and Optimization for Knowledge Discovery, 54-67. doi:10.4018/978-1-930708-26-6.ch004

[4]. Aggarwal, C. C. (2015). Data classification: algorithms and applications. Boca Raton: CRC Press.

[5]. http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/


------